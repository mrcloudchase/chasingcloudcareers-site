"use strict";(self.webpackChunkchasingcloudcareers=self.webpackChunkchasingcloudcareers||[]).push([[240],{6319:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"cloud-engineering/multi-cloud-architecture","title":"Multi-Cloud Architecture","description":"Master the design and implementation of applications that span multiple cloud providers, leveraging the best services from AWS, Azure, and Google Cloud while avoiding vendor lock-in.","source":"@site/docs/cloud-engineering/05-multi-cloud-architecture.md","sourceDirName":"cloud-engineering","slug":"/cloud-engineering/multi-cloud-architecture","permalink":"/chasingcloudcareers-site/docs/cloud-engineering/multi-cloud-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/mrcloudchase/chasingcloudcareers-site/tree/main/docs/cloud-engineering/05-multi-cloud-architecture.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Container Orchestration","permalink":"/chasingcloudcareers-site/docs/cloud-engineering/container-orchestration"},"next":{"title":"DevOps Engineering","permalink":"/chasingcloudcareers-site/docs/category/devops-engineering"}}');var s=t(4848),a=t(8453);const i={sidebar_position:6},o="Multi-Cloud Architecture",c={},l=[{value:"Introduction to Multi-Cloud",id:"introduction-to-multi-cloud",level:2},{value:"Multi-Cloud vs Hybrid Cloud",id:"multi-cloud-vs-hybrid-cloud",level:3},{value:"Benefits of Multi-Cloud Architecture",id:"benefits-of-multi-cloud-architecture",level:3},{value:"Multi-Cloud Challenges",id:"multi-cloud-challenges",level:3},{value:"Multi-Cloud Design Patterns",id:"multi-cloud-design-patterns",level:2},{value:"1. Cloud-Agnostic Application Design",id:"1-cloud-agnostic-application-design",level:3},{value:"2. Data Distribution Strategy",id:"2-data-distribution-strategy",level:3},{value:"3. Service Mesh for Multi-Cloud",id:"3-service-mesh-for-multi-cloud",level:3},{value:"Multi-Cloud Infrastructure with Terraform",id:"multi-cloud-infrastructure-with-terraform",level:2},{value:"Provider Configuration",id:"provider-configuration",level:3},{value:"Multi-Cloud Kubernetes Clusters",id:"multi-cloud-kubernetes-clusters",level:3},{value:"Multi-Cloud Load Balancing",id:"multi-cloud-load-balancing",level:3},{value:"Multi-Cloud Monitoring and Observability",id:"multi-cloud-monitoring-and-observability",level:2},{value:"Centralized Monitoring with Prometheus",id:"centralized-monitoring-with-prometheus",level:3},{value:"Distributed Tracing with Jaeger",id:"distributed-tracing-with-jaeger",level:3},{value:"Application Instrumentation",id:"application-instrumentation",level:3},{value:"Multi-Cloud Security",id:"multi-cloud-security",level:2},{value:"Identity Federation",id:"identity-federation",level:3},{value:"Secret Management Across Clouds",id:"secret-management-across-clouds",level:3},{value:"Multi-Cloud Cost Optimization",id:"multi-cloud-cost-optimization",level:2},{value:"Cost Monitoring and Analysis",id:"cost-monitoring-and-analysis",level:3},{value:"Automated Cost Optimization",id:"automated-cost-optimization",level:3},{value:"Free Learning Resources",id:"free-learning-resources",level:2},{value:"Multi-Cloud Platforms",id:"multi-cloud-platforms",level:3},{value:"Cloud Provider Documentation",id:"cloud-provider-documentation",level:3},{value:"Monitoring and Observability",id:"monitoring-and-observability",level:3},{value:"Practice and Certification",id:"practice-and-certification",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"multi-cloud-architecture",children:"Multi-Cloud Architecture"})}),"\n",(0,s.jsx)(n.p,{children:"Master the design and implementation of applications that span multiple cloud providers, leveraging the best services from AWS, Azure, and Google Cloud while avoiding vendor lock-in."}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-multi-cloud",children:"Introduction to Multi-Cloud"}),"\n",(0,s.jsx)(n.p,{children:"Multi-cloud architecture involves using services from multiple cloud providers simultaneously to optimize performance, cost, reliability, and avoid vendor lock-in."}),"\n",(0,s.jsx)(n.h3,{id:"multi-cloud-vs-hybrid-cloud",children:"Multi-Cloud vs Hybrid Cloud"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Multi-Cloud:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Uses multiple public cloud providers\n# Example: AWS for compute, GCP for AI/ML, Azure for enterprise integration\n# Applications distributed across different clouds\n# No on-premises infrastructure required\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Hybrid Cloud:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Combines on-premises infrastructure with public cloud\n# Example: On-premises data center + AWS cloud\n# Data and applications can move between environments\n# Maintains some on-premises control\n"})}),"\n",(0,s.jsx)(n.h3,{id:"benefits-of-multi-cloud-architecture",children:"Benefits of Multi-Cloud Architecture"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Avoid Vendor Lock-in:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Reduces dependency on single cloud provider\n# Freedom to choose best services from each provider\n# Negotiating power with cloud vendors\n# Protection against provider-specific outages\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Best-of-Breed Services:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# AWS: Mature compute and storage services\n# Google Cloud: Advanced AI/ML and data analytics\n# Azure: Enterprise integration and hybrid capabilities\n# Alibaba Cloud: Strong presence in Asia-Pacific\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Geographic Distribution:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Leverage different providers' regional strengths\n# Comply with data sovereignty requirements\n# Optimize latency for global users\n# Disaster recovery across providers\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cost Optimization:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Compare pricing across providers\n# Use spot instances and reserved capacity strategically\n# Optimize for different workload characteristics\n# Avoid egress charges through smart architecture\n"})}),"\n",(0,s.jsx)(n.h3,{id:"multi-cloud-challenges",children:"Multi-Cloud Challenges"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Complexity:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Multiple APIs and management interfaces\n# Different service models and pricing\n# Increased operational overhead\n# Need for specialized skills across platforms\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Data Transfer Costs:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Egress charges between clouds\n# Network latency between providers\n# Data synchronization challenges\n# Bandwidth limitations\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Security and Compliance:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Consistent security policies across clouds\n# Identity and access management complexity\n# Compliance with multiple frameworks\n# Monitoring and auditing challenges\n"})}),"\n",(0,s.jsx)(n.h2,{id:"multi-cloud-design-patterns",children:"Multi-Cloud Design Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"1-cloud-agnostic-application-design",children:"1. Cloud-Agnostic Application Design"}),"\n",(0,s.jsx)(n.p,{children:"Design applications that can run on any cloud provider with minimal changes."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Containerized Applications:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'# Use containers for portability\nFROM node:16-alpine\n\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\n\nCOPY . .\n\n# Use environment variables for configuration\nENV PORT=3000\nENV DATABASE_URL=""\nENV REDIS_URL=""\nENV STORAGE_BUCKET=""\n\nEXPOSE 3000\nCMD ["npm", "start"]\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Configuration Management:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// config.js - Cloud-agnostic configuration\nconst config = {\n  port: process.env.PORT || 3000,\n  database: {\n    url: process.env.DATABASE_URL,\n    ssl: process.env.NODE_ENV === 'production'\n  },\n  storage: {\n    bucket: process.env.STORAGE_BUCKET,\n    region: process.env.STORAGE_REGION\n  },\n  cache: {\n    url: process.env.REDIS_URL || 'redis://localhost:6379'\n  },\n  monitoring: {\n    endpoint: process.env.MONITORING_ENDPOINT,\n    apiKey: process.env.MONITORING_API_KEY\n  }\n};\n\nmodule.exports = config;\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Abstraction Layer for Cloud Services:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// storage-service.js - Abstract storage interface\nclass StorageService {\n  constructor(provider, config) {\n    this.provider = provider;\n    this.config = config;\n    this.client = this.initializeClient();\n  }\n\n  initializeClient() {\n    switch (this.provider) {\n      case 'aws':\n        const AWS = require('aws-sdk');\n        return new AWS.S3(this.config);\n      case 'gcp':\n        const { Storage } = require('@google-cloud/storage');\n        return new Storage(this.config);\n      case 'azure':\n        const { BlobServiceClient } = require('@azure/storage-blob');\n        return BlobServiceClient.fromConnectionString(this.config.connectionString);\n      default:\n        throw new Error(`Unsupported provider: ${this.provider}`);\n    }\n  }\n\n  async uploadFile(bucket, key, data) {\n    switch (this.provider) {\n      case 'aws':\n        return this.client.upload({ Bucket: bucket, Key: key, Body: data }).promise();\n      case 'gcp':\n        const file = this.client.bucket(bucket).file(key);\n        return file.save(data);\n      case 'azure':\n        const containerClient = this.client.getContainerClient(bucket);\n        const blockBlobClient = containerClient.getBlockBlobClient(key);\n        return blockBlobClient.upload(data, data.length);\n    }\n  }\n\n  async downloadFile(bucket, key) {\n    switch (this.provider) {\n      case 'aws':\n        const result = await this.client.getObject({ Bucket: bucket, Key: key }).promise();\n        return result.Body;\n      case 'gcp':\n        const file = this.client.bucket(bucket).file(key);\n        const [data] = await file.download();\n        return data;\n      case 'azure':\n        const containerClient = this.client.getContainerClient(bucket);\n        const blockBlobClient = containerClient.getBlockBlobClient(key);\n        const downloadResponse = await blockBlobClient.download();\n        return downloadResponse.readableStreamBody;\n    }\n  }\n}\n\nmodule.exports = StorageService;\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-data-distribution-strategy",children:"2. Data Distribution Strategy"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Geographic Data Distribution:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# Multi-region data strategy\nregions:\n  us-east:\n    provider: aws\n    services:\n      - compute: ec2\n      - storage: s3\n      - database: rds\n    users: ["north-america"]\n  \n  europe-west:\n    provider: gcp\n    services:\n      - compute: compute-engine\n      - storage: cloud-storage\n      - database: cloud-sql\n    users: ["europe", "africa"]\n  \n  asia-pacific:\n    provider: azure\n    services:\n      - compute: virtual-machines\n      - storage: blob-storage\n      - database: azure-sql\n    users: ["asia", "oceania"]\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Data Synchronization Architecture:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# data-sync-service.py\nimport asyncio\nfrom typing import Dict, List\nfrom dataclasses import dataclass\n\n@dataclass\nclass CloudProvider:\n    name: str\n    region: str\n    database_client: object\n    storage_client: object\n\nclass MultiCloudDataSync:\n    def __init__(self, providers: List[CloudProvider]):\n        self.providers = {p.name: p for p in providers}\n        self.sync_queue = asyncio.Queue()\n    \n    async def sync_data(self, data_type: str, data: Dict, target_regions: List[str]):\n        """Synchronize data across multiple cloud providers"""\n        tasks = []\n        \n        for region in target_regions:\n            if region in self.providers:\n                provider = self.providers[region]\n                task = self.write_to_provider(provider, data_type, data)\n                tasks.append(task)\n        \n        # Execute synchronization in parallel\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Handle failures and retry logic\n        failed_syncs = []\n        for i, result in enumerate(results):\n            if isinstance(result, Exception):\n                failed_syncs.append((target_regions[i], result))\n        \n        if failed_syncs:\n            await self.handle_sync_failures(failed_syncs, data_type, data)\n        \n        return len(results) - len(failed_syncs)\n    \n    async def write_to_provider(self, provider: CloudProvider, data_type: str, data: Dict):\n        """Write data to specific cloud provider"""\n        try:\n            if data_type == \'user_profile\':\n                await self.sync_user_profile(provider, data)\n            elif data_type == \'transaction\':\n                await self.sync_transaction(provider, data)\n            # Add more data types as needed\n        except Exception as e:\n            print(f"Failed to sync to {provider.name}: {e}")\n            raise\n    \n    async def sync_user_profile(self, provider: CloudProvider, data: Dict):\n        """Sync user profile data"""\n        # Implementation varies by provider\n        if provider.name == \'aws\':\n            # Use DynamoDB\n            await provider.database_client.put_item(\n                TableName=\'UserProfiles\',\n                Item=data\n            )\n        elif provider.name == \'gcp\':\n            # Use Firestore\n            await provider.database_client.collection(\'user_profiles\').document(data[\'id\']).set(data)\n        elif provider.name == \'azure\':\n            # Use Cosmos DB\n            await provider.database_client.create_item(data)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-service-mesh-for-multi-cloud",children:"3. Service Mesh for Multi-Cloud"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Istio Multi-Cloud Setup:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# istio-multicluster.yaml\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nmetadata:\n  name: control-plane\nspec:\n  values:\n    global:\n      meshID: mesh1\n      multiCluster:\n        clusterName: aws-cluster\n      network: aws-network\n  components:\n    pilot:\n      k8s:\n        env:\n          - name: PILOT_ENABLE_WORKLOAD_ENTRY_AUTOREGISTRATION\n            value: true\n          - name: PILOT_ENABLE_CROSS_CLUSTER_WORKLOAD_ENTRY\n            value: true\n\n---\n# Cross-cluster service discovery\napiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: gcp-service\nspec:\n  hosts:\n  - api.gcp.example.com\n  ports:\n  - number: 443\n    name: https\n    protocol: HTTPS\n  location: MESH_EXTERNAL\n  resolution: DNS\n\n---\n# Virtual service for traffic routing\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: multi-cloud-routing\nspec:\n  hosts:\n  - api.example.com\n  http:\n  - match:\n    - headers:\n        region:\n          exact: "us-east"\n    route:\n    - destination:\n        host: api.aws.example.com\n  - match:\n    - headers:\n        region:\n          exact: "europe"\n    route:\n    - destination:\n        host: api.gcp.example.com\n  - route:\n    - destination:\n        host: api.aws.example.com\n      weight: 70\n    - destination:\n        host: api.gcp.example.com\n      weight: 30\n'})}),"\n",(0,s.jsx)(n.h2,{id:"multi-cloud-infrastructure-with-terraform",children:"Multi-Cloud Infrastructure with Terraform"}),"\n",(0,s.jsx)(n.h3,{id:"provider-configuration",children:"Provider Configuration"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Multi-Provider Terraform Setup:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hcl",children:'# providers.tf\nterraform {\n  required_version = ">= 1.0"\n  required_providers {\n    aws = {\n      source  = "hashicorp/aws"\n      version = "~> 5.0"\n    }\n    google = {\n      source  = "hashicorp/google"\n      version = "~> 4.0"\n    }\n    azurerm = {\n      source  = "hashicorp/azurerm"\n      version = "~> 3.0"\n    }\n  }\n}\n\n# AWS Provider\nprovider "aws" {\n  region = var.aws_region\n  alias  = "us_east"\n}\n\nprovider "aws" {\n  region = var.aws_region_west\n  alias  = "us_west"\n}\n\n# Google Cloud Provider\nprovider "google" {\n  project = var.gcp_project\n  region  = var.gcp_region\n  alias   = "gcp_us"\n}\n\nprovider "google" {\n  project = var.gcp_project\n  region  = var.gcp_region_eu\n  alias   = "gcp_eu"\n}\n\n# Azure Provider\nprovider "azurerm" {\n  features {}\n  subscription_id = var.azure_subscription_id\n  alias          = "azure_east"\n}\n\nprovider "azurerm" {\n  features {}\n  subscription_id = var.azure_subscription_id\n  alias          = "azure_west"\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Variables Configuration:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hcl",children:'# variables.tf\nvariable "aws_region" {\n  description = "AWS primary region"\n  type        = string\n  default     = "us-east-1"\n}\n\nvariable "aws_region_west" {\n  description = "AWS secondary region"\n  type        = string\n  default     = "us-west-2"\n}\n\nvariable "gcp_project" {\n  description = "GCP project ID"\n  type        = string\n}\n\nvariable "gcp_region" {\n  description = "GCP primary region"\n  type        = string\n  default     = "us-central1"\n}\n\nvariable "gcp_region_eu" {\n  description = "GCP Europe region"\n  type        = string\n  default     = "europe-west1"\n}\n\nvariable "azure_subscription_id" {\n  description = "Azure subscription ID"\n  type        = string\n}\n\nvariable "environment" {\n  description = "Environment name"\n  type        = string\n  default     = "production"\n}\n\nvariable "application_name" {\n  description = "Application name"\n  type        = string\n  default     = "multicloud-app"\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"multi-cloud-kubernetes-clusters",children:"Multi-Cloud Kubernetes Clusters"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"AWS EKS Cluster:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hcl",children:'# aws-eks.tf\nmodule "aws_vpc" {\n  source = "terraform-aws-modules/vpc/aws"\n  \n  providers = {\n    aws = aws.us_east\n  }\n\n  name = "${var.application_name}-aws-vpc"\n  cidr = "10.0.0.0/16"\n\n  azs             = ["us-east-1a", "us-east-1b", "us-east-1c"]\n  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]\n  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]\n\n  enable_nat_gateway = true\n  enable_vpn_gateway = true\n\n  tags = {\n    Environment = var.environment\n    Cloud       = "aws"\n  }\n}\n\nmodule "aws_eks" {\n  source = "terraform-aws-modules/eks/aws"\n  \n  providers = {\n    aws = aws.us_east\n  }\n\n  cluster_name    = "${var.application_name}-aws-cluster"\n  cluster_version = "1.24"\n\n  vpc_id     = module.aws_vpc.vpc_id\n  subnet_ids = module.aws_vpc.private_subnets\n\n  eks_managed_node_groups = {\n    main = {\n      name = "main"\n      \n      instance_types = ["t3.medium"]\n      \n      min_size     = 2\n      max_size     = 10\n      desired_size = 3\n      \n      labels = {\n        Environment = var.environment\n        Cloud       = "aws"\n      }\n    }\n  }\n\n  tags = {\n    Environment = var.environment\n    Cloud       = "aws"\n  }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"GCP GKE Cluster:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hcl",children:'# gcp-gke.tf\nresource "google_compute_network" "gcp_vpc" {\n  provider = google.gcp_us\n  \n  name                    = "${var.application_name}-gcp-vpc"\n  auto_create_subnetworks = false\n}\n\nresource "google_compute_subnetwork" "gcp_subnet" {\n  provider = google.gcp_us\n  \n  name          = "${var.application_name}-gcp-subnet"\n  ip_cidr_range = "10.1.0.0/16"\n  region        = var.gcp_region\n  network       = google_compute_network.gcp_vpc.id\n\n  secondary_ip_range {\n    range_name    = "pods"\n    ip_cidr_range = "10.2.0.0/16"\n  }\n\n  secondary_ip_range {\n    range_name    = "services"\n    ip_cidr_range = "10.3.0.0/16"\n  }\n}\n\nresource "google_container_cluster" "gcp_cluster" {\n  provider = google.gcp_us\n  \n  name     = "${var.application_name}-gcp-cluster"\n  location = var.gcp_region\n\n  network    = google_compute_network.gcp_vpc.name\n  subnetwork = google_compute_subnetwork.gcp_subnet.name\n\n  # We can\'t create a cluster with no node pool defined, but we want to only use\n  # separately managed node pools. So we create the smallest possible default\n  # node pool and immediately delete it.\n  remove_default_node_pool = true\n  initial_node_count       = 1\n\n  ip_allocation_policy {\n    cluster_secondary_range_name  = "pods"\n    services_secondary_range_name = "services"\n  }\n\n  workload_identity_config {\n    workload_pool = "${var.gcp_project}.svc.id.goog"\n  }\n}\n\nresource "google_container_node_pool" "gcp_nodes" {\n  provider = google.gcp_us\n  \n  name       = "${var.application_name}-gcp-nodes"\n  location   = var.gcp_region\n  cluster    = google_container_cluster.gcp_cluster.name\n  node_count = 3\n\n  node_config {\n    preemptible  = false\n    machine_type = "e2-medium"\n\n    service_account = google_service_account.gke_service_account.email\n    oauth_scopes = [\n      "https://www.googleapis.com/auth/cloud-platform"\n    ]\n\n    labels = {\n      environment = var.environment\n      cloud       = "gcp"\n    }\n\n    tags = ["gke-node", "${var.application_name}-gke"]\n  }\n\n  autoscaling {\n    min_node_count = 1\n    max_node_count = 10\n  }\n\n  management {\n    auto_repair  = true\n    auto_upgrade = true\n  }\n}\n\nresource "google_service_account" "gke_service_account" {\n  provider = google.gcp_us\n  \n  account_id   = "${var.application_name}-gke-sa"\n  display_name = "GKE Service Account"\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Azure AKS Cluster:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hcl",children:'# azure-aks.tf\nresource "azurerm_resource_group" "azure_rg" {\n  provider = azurerm.azure_east\n  \n  name     = "${var.application_name}-azure-rg"\n  location = "East US"\n\n  tags = {\n    Environment = var.environment\n    Cloud       = "azure"\n  }\n}\n\nresource "azurerm_virtual_network" "azure_vnet" {\n  provider = azurerm.azure_east\n  \n  name                = "${var.application_name}-azure-vnet"\n  address_space       = ["10.4.0.0/16"]\n  location            = azurerm_resource_group.azure_rg.location\n  resource_group_name = azurerm_resource_group.azure_rg.name\n\n  tags = {\n    Environment = var.environment\n    Cloud       = "azure"\n  }\n}\n\nresource "azurerm_subnet" "azure_subnet" {\n  provider = azurerm.azure_east\n  \n  name                 = "${var.application_name}-azure-subnet"\n  resource_group_name  = azurerm_resource_group.azure_rg.name\n  virtual_network_name = azurerm_virtual_network.azure_vnet.name\n  address_prefixes     = ["10.4.1.0/24"]\n}\n\nresource "azurerm_kubernetes_cluster" "azure_cluster" {\n  provider = azurerm.azure_east\n  \n  name                = "${var.application_name}-azure-cluster"\n  location            = azurerm_resource_group.azure_rg.location\n  resource_group_name = azurerm_resource_group.azure_rg.name\n  dns_prefix          = "${var.application_name}-azure"\n\n  default_node_pool {\n    name           = "default"\n    node_count     = 3\n    vm_size        = "Standard_D2_v2"\n    vnet_subnet_id = azurerm_subnet.azure_subnet.id\n\n    enable_auto_scaling = true\n    min_count          = 1\n    max_count          = 10\n  }\n\n  identity {\n    type = "SystemAssigned"\n  }\n\n  network_profile {\n    network_plugin    = "azure"\n    load_balancer_sku = "standard"\n  }\n\n  tags = {\n    Environment = var.environment\n    Cloud       = "azure"\n  }\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"multi-cloud-load-balancing",children:"Multi-Cloud Load Balancing"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Global Load Balancer with Terraform:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hcl",children:'# global-load-balancer.tf\n\n# AWS Application Load Balancer\nresource "aws_lb" "aws_alb" {\n  provider = aws.us_east\n  \n  name               = "${var.application_name}-aws-alb"\n  internal           = false\n  load_balancer_type = "application"\n  security_groups    = [aws_security_group.alb_sg.id]\n  subnets            = module.aws_vpc.public_subnets\n\n  enable_deletion_protection = false\n\n  tags = {\n    Environment = var.environment\n    Cloud       = "aws"\n  }\n}\n\n# Google Cloud Load Balancer\nresource "google_compute_global_address" "gcp_ip" {\n  provider = google.gcp_us\n  \n  name = "${var.application_name}-gcp-ip"\n}\n\nresource "google_compute_global_forwarding_rule" "gcp_forwarding_rule" {\n  provider = google.gcp_us\n  \n  name       = "${var.application_name}-gcp-forwarding-rule"\n  target     = google_compute_target_https_proxy.gcp_https_proxy.id\n  port_range = "443"\n  ip_address = google_compute_global_address.gcp_ip.address\n}\n\n# Azure Application Gateway\nresource "azurerm_public_ip" "azure_pip" {\n  provider = azurerm.azure_east\n  \n  name                = "${var.application_name}-azure-pip"\n  resource_group_name = azurerm_resource_group.azure_rg.name\n  location            = azurerm_resource_group.azure_rg.location\n  allocation_method   = "Static"\n  sku                = "Standard"\n\n  tags = {\n    Environment = var.environment\n    Cloud       = "azure"\n  }\n}\n\nresource "azurerm_application_gateway" "azure_appgw" {\n  provider = azurerm.azure_east\n  \n  name                = "${var.application_name}-azure-appgw"\n  resource_group_name = azurerm_resource_group.azure_rg.name\n  location            = azurerm_resource_group.azure_rg.location\n\n  sku {\n    name     = "Standard_v2"\n    tier     = "Standard_v2"\n    capacity = 2\n  }\n\n  gateway_ip_configuration {\n    name      = "gateway-ip-config"\n    subnet_id = azurerm_subnet.azure_appgw_subnet.id\n  }\n\n  frontend_port {\n    name = "frontend-port"\n    port = 443\n  }\n\n  frontend_ip_configuration {\n    name                 = "frontend-ip-config"\n    public_ip_address_id = azurerm_public_ip.azure_pip.id\n  }\n\n  backend_address_pool {\n    name = "backend-pool"\n  }\n\n  backend_http_settings {\n    name                  = "backend-http-settings"\n    cookie_based_affinity = "Disabled"\n    path                  = "/"\n    port                  = 80\n    protocol              = "Http"\n    request_timeout       = 60\n  }\n\n  http_listener {\n    name                           = "http-listener"\n    frontend_ip_configuration_name = "frontend-ip-config"\n    frontend_port_name             = "frontend-port"\n    protocol                       = "Https"\n    ssl_certificate_name           = "ssl-cert"\n  }\n\n  request_routing_rule {\n    name                       = "routing-rule"\n    rule_type                  = "Basic"\n    http_listener_name         = "http-listener"\n    backend_address_pool_name  = "backend-pool"\n    backend_http_settings_name = "backend-http-settings"\n  }\n\n  tags = {\n    Environment = var.environment\n    Cloud       = "azure"\n  }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"multi-cloud-monitoring-and-observability",children:"Multi-Cloud Monitoring and Observability"}),"\n",(0,s.jsx)(n.h3,{id:"centralized-monitoring-with-prometheus",children:"Centralized Monitoring with Prometheus"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Prometheus Federation Setup:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# prometheus-federation.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n      evaluation_interval: 15s\n\n    rule_files:\n      - \"rules/*.yml\"\n\n    scrape_configs:\n      # AWS EKS cluster metrics\n      - job_name: 'aws-cluster'\n        honor_labels: true\n        metrics_path: '/federate'\n        params:\n          'match[]':\n            - '{job=~\"kubernetes-.*\"}'\n            - '{__name__=~\"node_.*\"}'\n            - '{__name__=~\"container_.*\"}'\n        static_configs:\n          - targets:\n            - 'prometheus-aws.monitoring.svc.cluster.local:9090'\n\n      # GCP GKE cluster metrics\n      - job_name: 'gcp-cluster'\n        honor_labels: true\n        metrics_path: '/federate'\n        params:\n          'match[]':\n            - '{job=~\"kubernetes-.*\"}'\n            - '{__name__=~\"node_.*\"}'\n            - '{__name__=~\"container_.*\"}'\n        static_configs:\n          - targets:\n            - 'prometheus-gcp.monitoring.svc.cluster.local:9090'\n\n      # Azure AKS cluster metrics\n      - job_name: 'azure-cluster'\n        honor_labels: true\n        metrics_path: '/federate'\n        params:\n          'match[]':\n            - '{job=~\"kubernetes-.*\"}'\n            - '{__name__=~\"node_.*\"}'\n            - '{__name__=~\"container_.*\"}'\n        static_configs:\n          - targets:\n            - 'prometheus-azure.monitoring.svc.cluster.local:9090'\n\n      # Application metrics from all clouds\n      - job_name: 'multicloud-app'\n        kubernetes_sd_configs:\n          - role: endpoints\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n\n    alerting:\n      alertmanagers:\n        - static_configs:\n            - targets:\n              - alertmanager:9093\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-federation\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus-federation\n  template:\n    metadata:\n      labels:\n        app: prometheus-federation\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus:latest\n        ports:\n        - containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/prometheus\n        - name: storage\n          mountPath: /prometheus\n        args:\n          - '--config.file=/etc/prometheus/prometheus.yml'\n          - '--storage.tsdb.path=/prometheus'\n          - '--web.console.libraries=/etc/prometheus/console_libraries'\n          - '--web.console.templates=/etc/prometheus/consoles'\n          - '--storage.tsdb.retention.time=30d'\n          - '--web.enable-lifecycle'\n      volumes:\n      - name: config\n        configMap:\n          name: prometheus-config\n      - name: storage\n        persistentVolumeClaim:\n          claimName: prometheus-storage\n"})}),"\n",(0,s.jsx)(n.h3,{id:"distributed-tracing-with-jaeger",children:"Distributed Tracing with Jaeger"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Multi-Cloud Jaeger Setup:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# jaeger-multicloud.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-collector\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: jaeger-collector\n  template:\n    metadata:\n      labels:\n        app: jaeger-collector\n    spec:\n      containers:\n      - name: jaeger-collector\n        image: jaegertracing/jaeger-collector:latest\n        ports:\n        - containerPort: 14267\n        - containerPort: 14268\n        - containerPort: 9411\n        env:\n        - name: SPAN_STORAGE_TYPE\n          value: "elasticsearch"\n        - name: ES_SERVER_URLS\n          value: "http://elasticsearch:9200"\n        - name: COLLECTOR_ZIPKIN_HTTP_PORT\n          value: "9411"\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: jaeger-collector\nspec:\n  selector:\n    app: jaeger-collector\n  ports:\n  - name: grpc\n    port: 14250\n    targetPort: 14250\n  - name: http\n    port: 14268\n    targetPort: 14268\n  - name: zipkin\n    port: 9411\n    targetPort: 9411\n  type: LoadBalancer\n\n---\n# Jaeger Agent DaemonSet for each cluster\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: jaeger-agent\nspec:\n  selector:\n    matchLabels:\n      app: jaeger-agent\n  template:\n    metadata:\n      labels:\n        app: jaeger-agent\n    spec:\n      containers:\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent:latest\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        env:\n        - name: REPORTER_GRPC_HOST_PORT\n          value: "jaeger-collector:14250"\n        resources:\n          limits:\n            memory: "128Mi"\n            cpu: "100m"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"application-instrumentation",children:"Application Instrumentation"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"OpenTelemetry Multi-Cloud Setup:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// tracing.js - OpenTelemetry configuration\nconst { NodeSDK } = require('@opentelemetry/sdk-node');\nconst { Resource } = require('@opentelemetry/resources');\nconst { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');\nconst { JaegerExporter } = require('@opentelemetry/exporter-jaeger');\nconst { PrometheusExporter } = require('@opentelemetry/exporter-prometheus');\n\n// Detect cloud provider\nconst detectCloudProvider = () => {\n  if (process.env.AWS_REGION) return 'aws';\n  if (process.env.GOOGLE_CLOUD_PROJECT) return 'gcp';\n  if (process.env.AZURE_SUBSCRIPTION_ID) return 'azure';\n  return 'unknown';\n};\n\nconst cloudProvider = detectCloudProvider();\n\n// Configure tracing\nconst jaegerExporter = new JaegerExporter({\n  endpoint: process.env.JAEGER_ENDPOINT || 'http://jaeger-collector:14268/api/traces',\n});\n\n// Configure metrics\nconst prometheusExporter = new PrometheusExporter({\n  port: 9464,\n}, () => {\n  console.log('Prometheus metrics server started on port 9464');\n});\n\nconst sdk = new NodeSDK({\n  resource: new Resource({\n    [SemanticResourceAttributes.SERVICE_NAME]: 'multicloud-app',\n    [SemanticResourceAttributes.SERVICE_VERSION]: process.env.APP_VERSION || '1.0.0',\n    [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV || 'development',\n    'cloud.provider': cloudProvider,\n    'cloud.region': process.env.CLOUD_REGION || 'unknown',\n  }),\n  traceExporter: jaegerExporter,\n  metricReader: prometheusExporter,\n});\n\nsdk.start();\n\nmodule.exports = sdk;\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Application Code with Tracing:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"// app.js - Multi-cloud application with tracing\nconst express = require('express');\nconst { trace, metrics } = require('@opentelemetry/api');\nconst StorageService = require('./storage-service');\n\nconst app = express();\nconst tracer = trace.getTracer('multicloud-app');\nconst meter = metrics.getMeter('multicloud-app');\n\n// Metrics\nconst requestCounter = meter.createCounter('http_requests_total', {\n  description: 'Total number of HTTP requests',\n});\n\nconst requestDuration = meter.createHistogram('http_request_duration_seconds', {\n  description: 'Duration of HTTP requests in seconds',\n});\n\n// Initialize storage service based on cloud provider\nconst cloudProvider = process.env.CLOUD_PROVIDER || 'aws';\nconst storageConfig = {\n  aws: { region: process.env.AWS_REGION },\n  gcp: { projectId: process.env.GOOGLE_CLOUD_PROJECT },\n  azure: { connectionString: process.env.AZURE_STORAGE_CONNECTION_STRING }\n};\n\nconst storage = new StorageService(cloudProvider, storageConfig[cloudProvider]);\n\napp.use(express.json());\n\n// Middleware for tracing and metrics\napp.use((req, res, next) => {\n  const startTime = Date.now();\n  \n  const span = tracer.startSpan(`${req.method} ${req.path}`, {\n    attributes: {\n      'http.method': req.method,\n      'http.url': req.url,\n      'http.route': req.path,\n      'cloud.provider': cloudProvider,\n    },\n  });\n\n  req.span = span;\n\n  res.on('finish', () => {\n    const duration = (Date.now() - startTime) / 1000;\n    \n    span.setAttributes({\n      'http.status_code': res.statusCode,\n      'http.response_size': res.get('content-length') || 0,\n    });\n    \n    span.end();\n\n    // Record metrics\n    requestCounter.add(1, {\n      method: req.method,\n      status_code: res.statusCode,\n      cloud_provider: cloudProvider,\n    });\n\n    requestDuration.record(duration, {\n      method: req.method,\n      status_code: res.statusCode,\n      cloud_provider: cloudProvider,\n    });\n  });\n\n  next();\n});\n\n// API endpoints\napp.post('/upload', async (req, res) => {\n  const span = tracer.startSpan('upload_file', { parent: req.span });\n  \n  try {\n    const { filename, data } = req.body;\n    const bucket = process.env.STORAGE_BUCKET;\n    \n    span.setAttributes({\n      'file.name': filename,\n      'storage.bucket': bucket,\n      'cloud.provider': cloudProvider,\n    });\n\n    const result = await storage.uploadFile(bucket, filename, Buffer.from(data, 'base64'));\n    \n    span.setAttributes({\n      'upload.success': true,\n      'upload.size': Buffer.from(data, 'base64').length,\n    });\n\n    res.json({ success: true, result });\n  } catch (error) {\n    span.recordException(error);\n    span.setStatus({ code: 2, message: error.message });\n    res.status(500).json({ error: error.message });\n  } finally {\n    span.end();\n  }\n});\n\napp.get('/download/:filename', async (req, res) => {\n  const span = tracer.startSpan('download_file', { parent: req.span });\n  \n  try {\n    const { filename } = req.params;\n    const bucket = process.env.STORAGE_BUCKET;\n    \n    span.setAttributes({\n      'file.name': filename,\n      'storage.bucket': bucket,\n      'cloud.provider': cloudProvider,\n    });\n\n    const data = await storage.downloadFile(bucket, filename);\n    \n    span.setAttributes({\n      'download.success': true,\n      'download.size': data.length,\n    });\n\n    res.json({ success: true, data: data.toString('base64') });\n  } catch (error) {\n    span.recordException(error);\n    span.setStatus({ code: 2, message: error.message });\n    res.status(500).json({ error: error.message });\n  } finally {\n    span.end();\n  }\n});\n\napp.get('/health', (req, res) => {\n  res.json({\n    status: 'healthy',\n    cloud_provider: cloudProvider,\n    region: process.env.CLOUD_REGION,\n    timestamp: new Date().toISOString(),\n  });\n});\n\nconst port = process.env.PORT || 3000;\napp.listen(port, () => {\n  console.log(`Multi-cloud app running on port ${port} (${cloudProvider})`);\n});\n"})}),"\n",(0,s.jsx)(n.h2,{id:"multi-cloud-security",children:"Multi-Cloud Security"}),"\n",(0,s.jsx)(n.h3,{id:"identity-federation",children:"Identity Federation"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cross-Cloud Identity Setup:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# identity-federation.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: identity-config\ndata:\n  config.yaml: |\n    providers:\n      aws:\n        type: "oidc"\n        issuer: "https://oidc.eks.us-east-1.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E"\n        client_id: "kubernetes"\n        username_claim: "sub"\n        groups_claim: "groups"\n      \n      gcp:\n        type: "oidc"\n        issuer: "https://container.googleapis.com/v1/projects/PROJECT_ID/zones/ZONE/clusters/CLUSTER_NAME"\n        client_id: "kubernetes"\n        username_claim: "sub"\n        groups_claim: "groups"\n      \n      azure:\n        type: "oidc"\n        issuer: "https://sts.windows.net/TENANT_ID/"\n        client_id: "CLIENT_ID"\n        username_claim: "oid"\n        groups_claim: "groups"\n\n---\n# RBAC configuration for multi-cloud access\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: multicloud-admin\nrules:\n- apiGroups: ["*"]\n  resources: ["*"]\n  verbs: ["*"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: multicloud-admin-binding\nsubjects:\n- kind: User\n  name: "system:serviceaccount:kube-system:multicloud-admin"\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: multicloud-admin\n  apiGroup: rbac.authorization.k8s.io\n'})}),"\n",(0,s.jsx)(n.h3,{id:"secret-management-across-clouds",children:"Secret Management Across Clouds"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Multi-Cloud Secret Synchronization:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# secret-sync.py\nimport asyncio\nimport base64\nfrom typing import Dict, Any\nimport boto3\nfrom google.cloud import secretmanager\nfrom azure.keyvault.secrets import SecretClient\nfrom azure.identity import DefaultAzureCredential\n\nclass MultiCloudSecretManager:\n    def __init__(self):\n        # AWS Secrets Manager\n        self.aws_client = boto3.client('secretsmanager')\n        \n        # Google Secret Manager\n        self.gcp_client = secretmanager.SecretManagerServiceClient()\n        \n        # Azure Key Vault\n        credential = DefaultAzureCredential()\n        vault_url = \"https://your-keyvault.vault.azure.net/\"\n        self.azure_client = SecretClient(vault_url=vault_url, credential=credential)\n    \n    async def sync_secret(self, secret_name: str, secret_value: str, target_clouds: list):\n        \"\"\"Synchronize secret across multiple cloud providers\"\"\"\n        tasks = []\n        \n        for cloud in target_clouds:\n            if cloud == 'aws':\n                tasks.append(self.store_aws_secret(secret_name, secret_value))\n            elif cloud == 'gcp':\n                tasks.append(self.store_gcp_secret(secret_name, secret_value))\n            elif cloud == 'azure':\n                tasks.append(self.store_azure_secret(secret_name, secret_value))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return results\n    \n    async def store_aws_secret(self, name: str, value: str):\n        \"\"\"Store secret in AWS Secrets Manager\"\"\"\n        try:\n            response = self.aws_client.create_secret(\n                Name=name,\n                SecretString=value,\n                Description=f'Multi-cloud synchronized secret: {name}'\n            )\n            return {'provider': 'aws', 'success': True, 'arn': response['ARN']}\n        except self.aws_client.exceptions.ResourceExistsException:\n            # Update existing secret\n            response = self.aws_client.update_secret(\n                SecretId=name,\n                SecretString=value\n            )\n            return {'provider': 'aws', 'success': True, 'arn': response['ARN']}\n        except Exception as e:\n            return {'provider': 'aws', 'success': False, 'error': str(e)}\n    \n    async def store_gcp_secret(self, name: str, value: str):\n        \"\"\"Store secret in Google Secret Manager\"\"\"\n        try:\n            project_id = \"your-project-id\"\n            parent = f\"projects/{project_id}\"\n            \n            # Create secret\n            secret = self.gcp_client.create_secret(\n                request={\n                    \"parent\": parent,\n                    \"secret_id\": name,\n                    \"secret\": {\"replication\": {\"automatic\": {}}},\n                }\n            )\n            \n            # Add secret version\n            response = self.gcp_client.add_secret_version(\n                request={\n                    \"parent\": secret.name,\n                    \"payload\": {\"data\": value.encode(\"UTF-8\")},\n                }\n            )\n            \n            return {'provider': 'gcp', 'success': True, 'name': response.name}\n        except Exception as e:\n            return {'provider': 'gcp', 'success': False, 'error': str(e)}\n    \n    async def store_azure_secret(self, name: str, value: str):\n        \"\"\"Store secret in Azure Key Vault\"\"\"\n        try:\n            secret = self.azure_client.set_secret(name, value)\n            return {'provider': 'azure', 'success': True, 'id': secret.id}\n        except Exception as e:\n            return {'provider': 'azure', 'success': False, 'error': str(e)}\n    \n    async def retrieve_secret(self, secret_name: str, cloud_provider: str):\n        \"\"\"Retrieve secret from specific cloud provider\"\"\"\n        if cloud_provider == 'aws':\n            response = self.aws_client.get_secret_value(SecretId=secret_name)\n            return response['SecretString']\n        elif cloud_provider == 'gcp':\n            project_id = \"your-project-id\"\n            name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\"\n            response = self.gcp_client.access_secret_version(request={\"name\": name})\n            return response.payload.data.decode(\"UTF-8\")\n        elif cloud_provider == 'azure':\n            secret = self.azure_client.get_secret(secret_name)\n            return secret.value\n\n# Usage example\nasync def main():\n    secret_manager = MultiCloudSecretManager()\n    \n    # Sync database password across all clouds\n    results = await secret_manager.sync_secret(\n        \"database-password\",\n        \"super-secure-password-123\",\n        [\"aws\", \"gcp\", \"azure\"]\n    )\n    \n    print(\"Secret synchronization results:\", results)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"})}),"\n",(0,s.jsx)(n.h2,{id:"multi-cloud-cost-optimization",children:"Multi-Cloud Cost Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"cost-monitoring-and-analysis",children:"Cost Monitoring and Analysis"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Multi-Cloud Cost Dashboard:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# cost-analyzer.py\nimport boto3\nimport pandas as pd\nfrom google.cloud import billing\nfrom azure.mgmt.consumption import ConsumptionManagementClient\nfrom azure.identity import DefaultAzureCredential\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\nclass MultiCloudCostAnalyzer:\n    def __init__(self):\n        # AWS Cost Explorer\n        self.aws_ce = boto3.client('ce')\n        \n        # GCP Billing\n        self.gcp_billing = billing.CloudBillingClient()\n        \n        # Azure Consumption\n        credential = DefaultAzureCredential()\n        self.azure_consumption = ConsumptionManagementClient(\n            credential, \n            subscription_id=\"your-subscription-id\"\n        )\n    \n    def get_aws_costs(self, start_date: str, end_date: str):\n        \"\"\"Get AWS costs for date range\"\"\"\n        response = self.aws_ce.get_cost_and_usage(\n            TimePeriod={\n                'Start': start_date,\n                'End': end_date\n            },\n            Granularity='DAILY',\n            Metrics=['BlendedCost'],\n            GroupBy=[\n                {\n                    'Type': 'DIMENSION',\n                    'Key': 'SERVICE'\n                }\n            ]\n        )\n        \n        costs = []\n        for result in response['ResultsByTime']:\n            date = result['TimePeriod']['Start']\n            for group in result['Groups']:\n                service = group['Keys'][0]\n                amount = float(group['Metrics']['BlendedCost']['Amount'])\n                costs.append({\n                    'date': date,\n                    'provider': 'aws',\n                    'service': service,\n                    'cost': amount\n                })\n        \n        return costs\n    \n    def get_gcp_costs(self, project_id: str, start_date: str, end_date: str):\n        \"\"\"Get GCP costs for date range\"\"\"\n        # Note: This requires BigQuery export of billing data\n        # Implementation would query BigQuery billing export table\n        \n        # Placeholder implementation\n        costs = []\n        # Query BigQuery billing export\n        # SELECT service.description, usage_start_time, cost\n        # FROM `project.dataset.gcp_billing_export_v1_BILLING_ACCOUNT_ID`\n        # WHERE usage_start_time >= start_date AND usage_start_time < end_date\n        \n        return costs\n    \n    def get_azure_costs(self, start_date: str, end_date: str):\n        \"\"\"Get Azure costs for date range\"\"\"\n        usage_details = self.azure_consumption.usage_details.list(\n            filter=f\"properties/usageStart ge '{start_date}' and properties/usageEnd le '{end_date}'\"\n        )\n        \n        costs = []\n        for usage in usage_details:\n            costs.append({\n                'date': usage.usage_start.strftime('%Y-%m-%d'),\n                'provider': 'azure',\n                'service': usage.meter_category,\n                'cost': usage.cost\n            })\n        \n        return costs\n    \n    def generate_cost_report(self, days: int = 30):\n        \"\"\"Generate comprehensive multi-cloud cost report\"\"\"\n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=days)\n        \n        start_str = start_date.strftime('%Y-%m-%d')\n        end_str = end_date.strftime('%Y-%m-%d')\n        \n        # Collect costs from all providers\n        all_costs = []\n        \n        # AWS costs\n        aws_costs = self.get_aws_costs(start_str, end_str)\n        all_costs.extend(aws_costs)\n        \n        # GCP costs (if implemented)\n        # gcp_costs = self.get_gcp_costs('your-project-id', start_str, end_str)\n        # all_costs.extend(gcp_costs)\n        \n        # Azure costs\n        azure_costs = self.get_azure_costs(start_str, end_str)\n        all_costs.extend(azure_costs)\n        \n        # Create DataFrame for analysis\n        df = pd.DataFrame(all_costs)\n        \n        if not df.empty:\n            # Generate summary by provider\n            provider_summary = df.groupby('provider')['cost'].sum().sort_values(ascending=False)\n            \n            # Generate summary by service\n            service_summary = df.groupby(['provider', 'service'])['cost'].sum().sort_values(ascending=False)\n            \n            # Generate daily trend\n            daily_trend = df.groupby(['date', 'provider'])['cost'].sum().unstack(fill_value=0)\n            \n            return {\n                'provider_summary': provider_summary,\n                'service_summary': service_summary,\n                'daily_trend': daily_trend,\n                'total_cost': df['cost'].sum()\n            }\n        \n        return None\n    \n    def create_cost_dashboard(self, report_data):\n        \"\"\"Create visual cost dashboard\"\"\"\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n        \n        # Provider cost pie chart\n        report_data['provider_summary'].plot(kind='pie', ax=ax1, autopct='%1.1f%%')\n        ax1.set_title('Cost Distribution by Cloud Provider')\n        \n        # Daily trend line chart\n        report_data['daily_trend'].plot(kind='line', ax=ax2)\n        ax2.set_title('Daily Cost Trend by Provider')\n        ax2.set_xlabel('Date')\n        ax2.set_ylabel('Cost ($)')\n        \n        # Top services bar chart\n        top_services = report_data['service_summary'].head(10)\n        top_services.plot(kind='barh', ax=ax3)\n        ax3.set_title('Top 10 Services by Cost')\n        ax3.set_xlabel('Cost ($)')\n        \n        # Cost optimization recommendations\n        ax4.text(0.1, 0.9, 'Cost Optimization Recommendations:', fontsize=12, fontweight='bold')\n        recommendations = [\n            '\u2022 Consider reserved instances for steady workloads',\n            '\u2022 Review unused resources and storage',\n            '\u2022 Implement auto-scaling policies',\n            '\u2022 Use spot instances for fault-tolerant workloads',\n            '\u2022 Optimize data transfer between regions',\n            '\u2022 Review and rightsize instance types'\n        ]\n        \n        for i, rec in enumerate(recommendations):\n            ax4.text(0.1, 0.8 - i*0.1, rec, fontsize=10)\n        \n        ax4.set_xlim(0, 1)\n        ax4.set_ylim(0, 1)\n        ax4.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig('multicloud_cost_dashboard.png', dpi=300, bbox_inches='tight')\n        plt.show()\n\n# Usage\nif __name__ == \"__main__\":\n    analyzer = MultiCloudCostAnalyzer()\n    report = analyzer.generate_cost_report(30)\n    \n    if report:\n        print(f\"Total 30-day cost: ${report['total_cost']:.2f}\")\n        print(\"\\nCost by provider:\")\n        print(report['provider_summary'])\n        \n        analyzer.create_cost_dashboard(report)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"automated-cost-optimization",children:"Automated Cost Optimization"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Multi-Cloud Resource Optimizer:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# resource-optimizer.py\nimport boto3\nfrom google.cloud import compute_v1\nfrom azure.mgmt.compute import ComputeManagementClient\nfrom azure.identity import DefaultAzureCredential\nfrom datetime import datetime, timedelta\nimport logging\n\nclass MultiCloudOptimizer:\n    def __init__(self):\n        # AWS clients\n        self.aws_ec2 = boto3.client('ec2')\n        self.aws_cloudwatch = boto3.client('cloudwatch')\n        \n        # GCP clients\n        self.gcp_compute = compute_v1.InstancesClient()\n        \n        # Azure clients\n        credential = DefaultAzureCredential()\n        self.azure_compute = ComputeManagementClient(\n            credential, \n            subscription_id=\"your-subscription-id\"\n        )\n        \n        self.logger = logging.getLogger(__name__)\n    \n    def find_idle_aws_instances(self, cpu_threshold: float = 5.0, days: int = 7):\n        \"\"\"Find idle AWS EC2 instances based on CPU utilization\"\"\"\n        idle_instances = []\n        \n        # Get all running instances\n        response = self.aws_ec2.describe_instances(\n            Filters=[{'Name': 'instance-state-name', 'Values': ['running']}]\n        )\n        \n        end_time = datetime.utcnow()\n        start_time = end_time - timedelta(days=days)\n        \n        for reservation in response['Reservations']:\n            for instance in reservation['Instances']:\n                instance_id = instance['InstanceId']\n                \n                # Get CPU utilization metrics\n                metrics = self.aws_cloudwatch.get_metric_statistics(\n                    Namespace='AWS/EC2',\n                    MetricName='CPUUtilization',\n                    Dimensions=[{'Name': 'InstanceId', 'Value': instance_id}],\n                    StartTime=start_time,\n                    EndTime=end_time,\n                    Period=3600,  # 1 hour\n                    Statistics=['Average']\n                )\n                \n                if metrics['Datapoints']:\n                    avg_cpu = sum(dp['Average'] for dp in metrics['Datapoints']) / len(metrics['Datapoints'])\n                    \n                    if avg_cpu < cpu_threshold:\n                        idle_instances.append({\n                            'instance_id': instance_id,\n                            'instance_type': instance['InstanceType'],\n                            'avg_cpu': avg_cpu,\n                            'provider': 'aws'\n                        })\n        \n        return idle_instances\n    \n    def find_unattached_aws_volumes(self):\n        \"\"\"Find unattached EBS volumes\"\"\"\n        response = self.aws_ec2.describe_volumes(\n            Filters=[{'Name': 'status', 'Values': ['available']}]\n        )\n        \n        unattached_volumes = []\n        for volume in response['Volumes']:\n            unattached_volumes.append({\n                'volume_id': volume['VolumeId'],\n                'size': volume['Size'],\n                'volume_type': volume['VolumeType'],\n                'provider': 'aws'\n            })\n        \n        return unattached_volumes\n    \n    def find_idle_gcp_instances(self, project_id: str, zone: str, cpu_threshold: float = 5.0):\n        \"\"\"Find idle GCP Compute Engine instances\"\"\"\n        # Note: This requires Cloud Monitoring API for CPU metrics\n        # Implementation would query Cloud Monitoring for CPU utilization\n        \n        idle_instances = []\n        \n        # List instances\n        request = compute_v1.ListInstancesRequest(\n            project=project_id,\n            zone=zone\n        )\n        \n        instances = self.gcp_compute.list(request=request)\n        \n        for instance in instances:\n            if instance.status == 'RUNNING':\n                # Query Cloud Monitoring for CPU metrics\n                # This is a placeholder - actual implementation would use Cloud Monitoring API\n                \n                idle_instances.append({\n                    'instance_id': instance.name,\n                    'machine_type': instance.machine_type.split('/')[-1],\n                    'provider': 'gcp',\n                    'zone': zone\n                })\n        \n        return idle_instances\n    \n    def generate_optimization_report(self):\n        \"\"\"Generate comprehensive optimization report\"\"\"\n        report = {\n            'timestamp': datetime.now().isoformat(),\n            'recommendations': []\n        }\n        \n        # AWS optimizations\n        idle_aws_instances = self.find_idle_aws_instances()\n        unattached_aws_volumes = self.find_unattached_aws_volumes()\n        \n        if idle_aws_instances:\n            report['recommendations'].append({\n                'type': 'idle_instances',\n                'provider': 'aws',\n                'count': len(idle_aws_instances),\n                'instances': idle_aws_instances,\n                'action': 'Consider stopping or downsizing these instances'\n            })\n        \n        if unattached_aws_volumes:\n            report['recommendations'].append({\n                'type': 'unattached_volumes',\n                'provider': 'aws',\n                'count': len(unattached_aws_volumes),\n                'volumes': unattached_aws_volumes,\n                'action': 'Consider deleting unused volumes or creating snapshots'\n            })\n        \n        # GCP optimizations (if implemented)\n        # idle_gcp_instances = self.find_idle_gcp_instances('your-project', 'us-central1-a')\n        \n        # Azure optimizations (similar implementation)\n        \n        return report\n    \n    def auto_optimize(self, dry_run: bool = True):\n        \"\"\"Automatically optimize resources based on policies\"\"\"\n        report = self.generate_optimization_report()\n        actions_taken = []\n        \n        for recommendation in report['recommendations']:\n            if recommendation['type'] == 'idle_instances' and recommendation['provider'] == 'aws':\n                for instance in recommendation['instances']:\n                    if instance['avg_cpu'] < 2.0:  # Very low CPU\n                        action = f\"Stop instance {instance['instance_id']} (CPU: {instance['avg_cpu']:.1f}%)\"\n                        \n                        if not dry_run:\n                            try:\n                                self.aws_ec2.stop_instances(InstanceIds=[instance['instance_id']])\n                                self.logger.info(f\"Stopped instance {instance['instance_id']}\")\n                            except Exception as e:\n                                self.logger.error(f\"Failed to stop instance {instance['instance_id']}: {e}\")\n                        \n                        actions_taken.append(action)\n            \n            elif recommendation['type'] == 'unattached_volumes' and recommendation['provider'] == 'aws':\n                for volume in recommendation['volumes']:\n                    if volume['size'] < 10:  # Small volumes\n                        action = f\"Delete volume {volume['volume_id']} ({volume['size']}GB)\"\n                        \n                        if not dry_run:\n                            try:\n                                # Create snapshot first\n                                snapshot = self.aws_ec2.create_snapshot(\n                                    VolumeId=volume['volume_id'],\n                                    Description=f\"Auto-backup before deletion - {datetime.now()}\"\n                                )\n                                \n                                # Delete volume\n                                self.aws_ec2.delete_volume(VolumeId=volume['volume_id'])\n                                self.logger.info(f\"Deleted volume {volume['volume_id']}, snapshot: {snapshot['SnapshotId']}\")\n                            except Exception as e:\n                                self.logger.error(f\"Failed to delete volume {volume['volume_id']}: {e}\")\n                        \n                        actions_taken.append(action)\n        \n        return {\n            'dry_run': dry_run,\n            'actions_taken': actions_taken,\n            'report': report\n        }\n\n# Usage\nif __name__ == \"__main__\":\n    optimizer = MultiCloudOptimizer()\n    \n    # Generate report\n    report = optimizer.generate_optimization_report()\n    print(\"Optimization Report:\")\n    for rec in report['recommendations']:\n        print(f\"- {rec['type']}: {rec['count']} items ({rec['provider']})\")\n    \n    # Dry run optimization\n    result = optimizer.auto_optimize(dry_run=True)\n    print(f\"\\nDry run completed. {len(result['actions_taken'])} actions would be taken:\")\n    for action in result['actions_taken']:\n        print(f\"- {action}\")\n"})}),"\n",(0,s.jsx)(n.h2,{id:"free-learning-resources",children:"Free Learning Resources"}),"\n",(0,s.jsx)(n.h3,{id:"multi-cloud-platforms",children:"Multi-Cloud Platforms"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.terraform.io/docs/providers/",children:"Terraform Multi-Cloud"})," - Multi-cloud infrastructure as code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.pulumi.com/",children:"Pulumi"})," - Modern infrastructure as code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://crossplane.io/",children:"Crossplane"})," - Kubernetes-based multi-cloud control plane"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"cloud-provider-documentation",children:"Cloud Provider Documentation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://aws.amazon.com/hybrid/",children:"AWS Multi-Cloud"})," - AWS hybrid and multi-cloud solutions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://cloud.google.com/anthos",children:"Google Anthos"})," - Google's multi-cloud platform"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://azure.microsoft.com/services/azure-arc/",children:"Azure Arc"})," - Azure multi-cloud management"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"monitoring-and-observability",children:"Monitoring and Observability"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://prometheus.io/",children:"Prometheus"})," - Multi-cloud monitoring"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://grafana.com/",children:"Grafana"})," - Multi-cloud dashboards"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.jaegertracing.io/",children:"Jaeger"})," - Distributed tracing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://opentelemetry.io/",children:"OpenTelemetry"})," - Observability framework"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"practice-and-certification",children:"Practice and Certification"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://www.cncf.io/",children:"Cloud Native Computing Foundation"})," - Cloud native technologies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://docs.microsoft.com/en-us/azure/architecture/",children:"Multi-Cloud Architecture Patterns"})," - Architecture guidance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://aws.amazon.com/architecture/well-architected/",children:"AWS Well-Architected Framework"})," - Best practices"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"After mastering multi-cloud architecture:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advanced Orchestration"}),": Service mesh, API gateways, event-driven architecture"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cloud Native Security"}),": Zero-trust architecture, policy as code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Edge Computing"}),": CDN, edge functions, IoT integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FinOps"}),": Advanced cost optimization and governance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Certification"}),": Multi-cloud architect certifications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Join Communities"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.cncf.io/community/",children:"Cloud Native Computing Foundation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.reddit.com/r/cloudcomputing/",children:"Multi-Cloud Forums"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Continue to ",(0,s.jsx)(n.strong,{children:"DevOps and CI/CD"})," to learn complete automation pipelines, or explore ",(0,s.jsx)(n.strong,{children:"Advanced Cloud Security"})," for enterprise-grade security implementations!"]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var r=t(6540);const s={},a=r.createContext(s);function i(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);