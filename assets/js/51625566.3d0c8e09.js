"use strict";(self.webpackChunkchasingcloudcareers=self.webpackChunkchasingcloudcareers||[]).push([[1987],{6473:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"devops-engineering/monitoring-security-advanced","title":"Monitoring, Security, and Advanced Practices","description":"Master comprehensive observability, implement enterprise security practices, and develop advanced DevOps capabilities for large-scale, mission-critical systems.","source":"@site/docs/devops-engineering/05-monitoring-security-advanced.md","sourceDirName":"devops-engineering","slug":"/devops-engineering/monitoring-security-advanced","permalink":"/chasingcloudcareers-site/docs/devops-engineering/monitoring-security-advanced","draft":false,"unlisted":false,"editUrl":"https://github.com/mrcloudchase/chasingcloudcareers-site/tree/main/docs/devops-engineering/05-monitoring-security-advanced.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Containerization and Orchestration","permalink":"/chasingcloudcareers-site/docs/devops-engineering/containerization-orchestration"},"next":{"title":"\ud83e\udd16 AI Engineering","permalink":"/chasingcloudcareers-site/docs/category/-ai-engineering"}}');var i=t(4848),s=t(8453);const a={sidebar_position:7},o="Monitoring, Security, and Advanced Practices",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1. Comprehensive Observability and Monitoring",id:"1-comprehensive-observability-and-monitoring",level:2},{value:"The Three Pillars of Observability",id:"the-three-pillars-of-observability",level:3},{value:"Free Resources",id:"free-resources",level:3},{value:"2. DevSecOps and Security Automation",id:"2-devsecops-and-security-automation",level:2},{value:"Security in the Development Lifecycle",id:"security-in-the-development-lifecycle",level:3},{value:"Secrets Management and Encryption",id:"secrets-management-and-encryption",level:3},{value:"Free Resources",id:"free-resources-1",level:3},{value:"3. Site Reliability Engineering (SRE)",id:"3-site-reliability-engineering-sre",level:2},{value:"SRE Principles and Practices",id:"sre-principles-and-practices",level:3},{value:"Incident Management and Post-Mortems",id:"incident-management-and-post-mortems",level:3},{value:"Free Resources",id:"free-resources-2",level:3},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: Complete Observability Stack",id:"exercise-1-complete-observability-stack",level:3},{value:"Exercise 2: DevSecOps Pipeline Implementation",id:"exercise-2-devsecops-pipeline-implementation",level:3},{value:"Exercise 3: SRE Practice Implementation",id:"exercise-3-sre-practice-implementation",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Congratulations! \ud83c\udf89",id:"congratulations-",level:2},{value:"Next Steps in Your DevOps Career",id:"next-steps-in-your-devops-career",level:2},{value:"<strong>Immediate Actions:</strong>",id:"immediate-actions",level:3},{value:"<strong>Career Advancement Opportunities:</strong>",id:"career-advancement-opportunities",level:3},{value:"<strong>Continuous Learning:</strong>",id:"continuous-learning",level:3},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Professional Development",id:"professional-development",level:3},{value:"Communities and Networking",id:"communities-and-networking",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"monitoring-security-and-advanced-practices",children:"Monitoring, Security, and Advanced Practices"})}),"\n",(0,i.jsx)(n.p,{children:"Master comprehensive observability, implement enterprise security practices, and develop advanced DevOps capabilities for large-scale, mission-critical systems."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this module, you will:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Design and implement comprehensive observability and monitoring solutions"}),"\n",(0,i.jsx)(n.li,{children:"Master DevSecOps practices and security automation throughout the development lifecycle"}),"\n",(0,i.jsx)(n.li,{children:"Apply Site Reliability Engineering (SRE) principles and practices"}),"\n",(0,i.jsx)(n.li,{children:"Optimize application and infrastructure performance at scale"}),"\n",(0,i.jsx)(n.li,{children:"Build advanced automation and workflow orchestration systems"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"1-comprehensive-observability-and-monitoring",children:"1. Comprehensive Observability and Monitoring"}),"\n",(0,i.jsx)(n.h3,{id:"the-three-pillars-of-observability",children:"The Three Pillars of Observability"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Metrics, Logs, and Traces Integration:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# observability-stack.yaml - Complete observability platform\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: observability\n\n---\n# Prometheus for metrics\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: observability\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n    spec:\n      serviceAccountName: prometheus\n      containers:\n      - name: prometheus\n        image: prom/prometheus:v2.45.0\n        ports:\n        - containerPort: 9090\n        args:\n        - \'--config.file=/etc/prometheus/prometheus.yml\'\n        - \'--storage.tsdb.path=/prometheus\'\n        - \'--web.console.libraries=/etc/prometheus/console_libraries\'\n        - \'--web.console.templates=/etc/prometheus/consoles\'\n        - \'--storage.tsdb.retention.time=30d\'\n        - \'--web.enable-lifecycle\'\n        - \'--web.enable-admin-api\'\n        - \'--storage.tsdb.wal-compression\'\n        volumeMounts:\n        - name: config\n          mountPath: /etc/prometheus\n        - name: storage\n          mountPath: /prometheus\n        resources:\n          requests:\n            memory: "2Gi"\n            cpu: "500m"\n          limits:\n            memory: "4Gi"\n            cpu: "1000m"\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n      volumes:\n      - name: config\n        configMap:\n          name: prometheus-config\n      - name: storage\n        persistentVolumeClaim:\n          claimName: prometheus-storage\n\n---\n# Grafana for visualization\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\n  namespace: observability\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: grafana/grafana:10.0.0\n        ports:\n        - containerPort: 3000\n        env:\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: grafana-secrets\n              key: admin-password\n        - name: GF_INSTALL_PLUGINS\n          value: "grafana-piechart-panel,grafana-worldmap-panel"\n        volumeMounts:\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: config\n          mountPath: /etc/grafana/provisioning\n        resources:\n          requests:\n            memory: "256Mi"\n            cpu: "100m"\n          limits:\n            memory: "512Mi"\n            cpu: "200m"\n      volumes:\n      - name: storage\n        persistentVolumeClaim:\n          claimName: grafana-storage\n      - name: config\n        configMap:\n          name: grafana-config\n\n---\n# Jaeger for distributed tracing\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger\n  namespace: observability\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jaeger\n  template:\n    metadata:\n      labels:\n        app: jaeger\n    spec:\n      containers:\n      - name: jaeger\n        image: jaegertracing/all-in-one:1.47\n        ports:\n        - containerPort: 16686\n        - containerPort: 14268\n        env:\n        - name: COLLECTOR_ZIPKIN_HOST_PORT\n          value: ":9411"\n        - name: SPAN_STORAGE_TYPE\n          value: "elasticsearch"\n        - name: ES_SERVER_URLS\n          value: "http://elasticsearch:9200"\n        resources:\n          requests:\n            memory: "256Mi"\n            cpu: "100m"\n          limits:\n            memory: "512Mi"\n            cpu: "200m"\n\n---\n# Elasticsearch for log storage\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch\n  namespace: observability\nspec:\n  serviceName: elasticsearch\n  replicas: 3\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n    spec:\n      containers:\n      - name: elasticsearch\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0\n        ports:\n        - containerPort: 9200\n        - containerPort: 9300\n        env:\n        - name: cluster.name\n          value: "observability-cluster"\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: discovery.seed_hosts\n          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"\n        - name: cluster.initial_master_nodes\n          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"\n        - name: ES_JAVA_OPTS\n          value: "-Xms1g -Xmx1g"\n        - name: xpack.security.enabled\n          value: "false"\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n        resources:\n          requests:\n            memory: "2Gi"\n            cpu: "500m"\n          limits:\n            memory: "4Gi"\n            cpu: "1000m"\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: ["ReadWriteOnce"]\n      resources:\n        requests:\n          storage: 10Gi\n\n---\n# Fluentd for log collection\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: observability\nspec:\n  selector:\n    matchLabels:\n      app: fluentd\n  template:\n    metadata:\n      labels:\n        app: fluentd\n    spec:\n      serviceAccountName: fluentd\n      containers:\n      - name: fluentd\n        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch\n        env:\n        - name: FLUENT_ELASTICSEARCH_HOST\n          value: "elasticsearch"\n        - name: FLUENT_ELASTICSEARCH_PORT\n          value: "9200"\n        - name: FLUENT_ELASTICSEARCH_SCHEME\n          value: "http"\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: config\n          mountPath: /fluentd/etc\n        resources:\n          requests:\n            memory: "200Mi"\n            cpu: "100m"\n          limits:\n            memory: "400Mi"\n            cpu: "200m"\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: config\n        configMap:\n          name: fluentd-config\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Advanced Monitoring Configuration:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# prometheus-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\n  namespace: observability\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n      evaluation_interval: 15s\n      external_labels:\n        cluster: \'production\'\n        region: \'us-west-2\'\n\n    rule_files:\n      - "/etc/prometheus/rules/*.yml"\n\n    alerting:\n      alertmanagers:\n      - static_configs:\n        - targets:\n          - alertmanager:9093\n\n    scrape_configs:\n    # Kubernetes API server\n    - job_name: \'kubernetes-apiservers\'\n      kubernetes_sd_configs:\n      - role: endpoints\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n        action: keep\n        regex: default;kubernetes;https\n\n    # Kubernetes nodes\n    - job_name: \'kubernetes-nodes\'\n      kubernetes_sd_configs:\n      - role: node\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/${1}/proxy/metrics\n\n    # Kubernetes pods\n    - job_name: \'kubernetes-pods\'\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n        target_label: kubernetes_pod_name\n\n    # Application metrics\n    - job_name: \'application-metrics\'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n        action: replace\n        target_label: __scheme__\n        regex: (https?)\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n        action: replace\n        target_label: __address__\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        action: replace\n        target_label: kubernetes_name\n\n  rules.yml: |\n    groups:\n    - name: kubernetes.rules\n      rules:\n      - alert: KubernetesNodeReady\n        expr: kube_node_status_condition{condition="Ready",status="true"} == 0\n        for: 10m\n        labels:\n          severity: critical\n        annotations:\n          summary: "Kubernetes Node not ready"\n          description: "Node {{ $labels.node }} has been unready for more than 10 minutes"\n\n      - alert: KubernetesPodCrashLooping\n        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: "Pod is crash looping"\n          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"\n\n      - alert: KubernetesHighCPUUsage\n        expr: (sum by (instance) (rate(container_cpu_usage_seconds_total[5m])) * 100) > 80\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: "High CPU usage detected"\n          description: "CPU usage is above 80% on {{ $labels.instance }}"\n\n      - alert: KubernetesHighMemoryUsage\n        expr: (sum by (instance) (container_memory_working_set_bytes) / sum by (instance) (container_spec_memory_limit_bytes) * 100) > 80\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: "High memory usage detected"\n          description: "Memory usage is above 80% on {{ $labels.instance }}"\n\n    - name: application.rules\n      rules:\n      - alert: ApplicationHighErrorRate\n        expr: (sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))) * 100 > 5\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: "High application error rate"\n          description: "Application error rate is above 5% for more than 5 minutes"\n\n      - alert: ApplicationHighLatency\n        expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 0.5\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: "High application latency"\n          description: "95th percentile latency is above 500ms for more than 5 minutes"\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Custom Metrics and Instrumentation:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nAdvanced application instrumentation with custom metrics\n"""\nimport time\nimport random\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom flask import Flask, request, jsonify\nimport logging\nimport structlog\n\n# Configure structured logging\nstructlog.configure(\n    processors=[\n        structlog.stdlib.filter_by_level,\n        structlog.stdlib.add_logger_name,\n        structlog.stdlib.add_log_level,\n        structlog.stdlib.PositionalArgumentsFormatter(),\n        structlog.processors.TimeStamper(fmt="iso"),\n        structlog.processors.StackInfoRenderer(),\n        structlog.processors.format_exc_info,\n        structlog.processors.UnicodeDecoder(),\n        structlog.processors.JSONRenderer()\n    ],\n    context_class=dict,\n    logger_factory=structlog.stdlib.LoggerFactory(),\n    wrapper_class=structlog.stdlib.BoundLogger,\n    cache_logger_on_first_use=True,\n)\n\nlogger = structlog.get_logger()\n\n# Configure OpenTelemetry tracing\ntrace.set_tracer_provider(TracerProvider())\ntracer = trace.get_tracer(__name__)\n\njaeger_exporter = JaegerExporter(\n    agent_host_name="jaeger",\n    agent_port=6831,\n)\n\nspan_processor = BatchSpanProcessor(jaeger_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Prometheus metrics\nREQUEST_COUNT = Counter(\n    \'http_requests_total\',\n    \'Total HTTP requests\',\n    [\'method\', \'endpoint\', \'status\']\n)\n\nREQUEST_LATENCY = Histogram(\n    \'http_request_duration_seconds\',\n    \'HTTP request latency\',\n    [\'method\', \'endpoint\']\n)\n\nACTIVE_CONNECTIONS = Gauge(\n    \'active_connections\',\n    \'Number of active connections\'\n)\n\nBUSINESS_METRICS = Counter(\n    \'business_events_total\',\n    \'Business events counter\',\n    [\'event_type\', \'user_type\']\n)\n\nDATABASE_OPERATIONS = Histogram(\n    \'database_operation_duration_seconds\',\n    \'Database operation duration\',\n    [\'operation\', \'table\']\n)\n\nCACHE_OPERATIONS = Counter(\n    \'cache_operations_total\',\n    \'Cache operations counter\',\n    [\'operation\', \'result\']\n)\n\n# Flask application\napp = Flask(__name__)\n\n# Instrument Flask and requests\nFlaskInstrumentor().instrument_app(app)\nRequestsInstrumentor().instrument()\n\nclass MetricsMiddleware:\n    def __init__(self, app):\n        self.app = app\n        self.app.before_request(self.before_request)\n        self.app.after_request(self.after_request)\n        \n    def before_request(self):\n        request.start_time = time.time()\n        ACTIVE_CONNECTIONS.inc()\n        \n        logger.info(\n            "Request started",\n            method=request.method,\n            path=request.path,\n            remote_addr=request.remote_addr,\n            user_agent=request.headers.get(\'User-Agent\')\n        )\n    \n    def after_request(self, response):\n        request_latency = time.time() - request.start_time\n        \n        REQUEST_COUNT.labels(\n            method=request.method,\n            endpoint=request.endpoint or \'unknown\',\n            status=response.status_code\n        ).inc()\n        \n        REQUEST_LATENCY.labels(\n            method=request.method,\n            endpoint=request.endpoint or \'unknown\'\n        ).observe(request_latency)\n        \n        ACTIVE_CONNECTIONS.dec()\n        \n        logger.info(\n            "Request completed",\n            method=request.method,\n            path=request.path,\n            status_code=response.status_code,\n            duration=request_latency,\n            response_size=response.content_length\n        )\n        \n        return response\n\n# Apply middleware\nMetricsMiddleware(app)\n\n@app.route(\'/health\')\ndef health():\n    """Health check endpoint"""\n    return jsonify({"status": "healthy", "timestamp": time.time()})\n\n@app.route(\'/ready\')\ndef ready():\n    """Readiness check endpoint"""\n    # Perform dependency checks here\n    dependencies = {\n        "database": check_database(),\n        "cache": check_cache(),\n        "external_api": check_external_api()\n    }\n    \n    all_ready = all(dependencies.values())\n    status_code = 200 if all_ready else 503\n    \n    return jsonify({\n        "status": "ready" if all_ready else "not ready",\n        "dependencies": dependencies,\n        "timestamp": time.time()\n    }), status_code\n\n@app.route(\'/api/users\', methods=[\'GET\', \'POST\'])\ndef users():\n    """Users API endpoint with custom metrics"""\n    with tracer.start_as_current_span("users_api") as span:\n        span.set_attribute("http.method", request.method)\n        span.set_attribute("http.url", request.url)\n        \n        if request.method == \'POST\':\n            # Simulate user creation\n            user_data = request.get_json()\n            user_type = user_data.get(\'type\', \'regular\')\n            \n            # Business metric\n            BUSINESS_METRICS.labels(\n                event_type=\'user_created\',\n                user_type=user_type\n            ).inc()\n            \n            # Simulate database operation\n            with tracer.start_as_current_span("database_insert") as db_span:\n                db_start = time.time()\n                simulate_database_operation(\'insert\', \'users\')\n                db_duration = time.time() - db_start\n                \n                DATABASE_OPERATIONS.labels(\n                    operation=\'insert\',\n                    table=\'users\'\n                ).observe(db_duration)\n                \n                db_span.set_attribute("db.operation", "insert")\n                db_span.set_attribute("db.table", "users")\n                db_span.set_attribute("db.duration", db_duration)\n            \n            logger.info(\n                "User created",\n                user_type=user_type,\n                user_id=user_data.get(\'id\'),\n                database_duration=db_duration\n            )\n            \n            return jsonify({"message": "User created", "id": user_data.get(\'id\')}), 201\n        \n        else:\n            # Simulate cache check\n            cache_start = time.time()\n            cache_hit = simulate_cache_operation(\'get\', \'users_list\')\n            cache_duration = time.time() - cache_start\n            \n            CACHE_OPERATIONS.labels(\n                operation=\'get\',\n                result=\'hit\' if cache_hit else \'miss\'\n            ).inc()\n            \n            if not cache_hit:\n                # Simulate database query\n                with tracer.start_as_current_span("database_select") as db_span:\n                    db_start = time.time()\n                    users = simulate_database_operation(\'select\', \'users\')\n                    db_duration = time.time() - db_start\n                    \n                    DATABASE_OPERATIONS.labels(\n                        operation=\'select\',\n                        table=\'users\'\n                    ).observe(db_duration)\n                    \n                    db_span.set_attribute("db.operation", "select")\n                    db_span.set_attribute("db.table", "users")\n                    db_span.set_attribute("db.rows_returned", len(users))\n            \n            logger.info(\n                "Users retrieved",\n                cache_hit=cache_hit,\n                cache_duration=cache_duration,\n                user_count=len(users) if not cache_hit else "cached"\n            )\n            \n            return jsonify({"users": users if not cache_hit else "cached_data"})\n\ndef check_database():\n    """Check database connectivity"""\n    try:\n        # Simulate database check\n        time.sleep(random.uniform(0.01, 0.05))\n        return random.choice([True, True, True, False])  # 75% success rate\n    except Exception as e:\n        logger.error("Database check failed", error=str(e))\n        return False\n\ndef check_cache():\n    """Check cache connectivity"""\n    try:\n        # Simulate cache check\n        time.sleep(random.uniform(0.005, 0.02))\n        return random.choice([True, True, True, True, False])  # 80% success rate\n    except Exception as e:\n        logger.error("Cache check failed", error=str(e))\n        return False\n\ndef check_external_api():\n    """Check external API connectivity"""\n    try:\n        # Simulate external API check\n        time.sleep(random.uniform(0.1, 0.3))\n        return random.choice([True, True, False])  # 66% success rate\n    except Exception as e:\n        logger.error("External API check failed", error=str(e))\n        return False\n\ndef simulate_database_operation(operation, table):\n    """Simulate database operation"""\n    # Simulate varying latency\n    latency = random.uniform(0.01, 0.1)\n    time.sleep(latency)\n    \n    if operation == \'select\':\n        return [{"id": i, "name": f"User {i}"} for i in range(1, 11)]\n    elif operation == \'insert\':\n        return {"id": random.randint(1000, 9999)}\n    \n    return None\n\ndef simulate_cache_operation(operation, key):\n    """Simulate cache operation"""\n    # Simulate cache hit/miss\n    time.sleep(random.uniform(0.001, 0.005))\n    return random.choice([True, False])  # 50% hit rate\n\n@app.route(\'/metrics\')\ndef metrics():\n    """Prometheus metrics endpoint"""\n    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST\n    return generate_latest(), 200, {\'Content-Type\': CONTENT_TYPE_LATEST}\n\nif __name__ == \'__main__\':\n    # Start Prometheus metrics server\n    start_http_server(8000)\n    \n    logger.info("Application starting", port=5000, metrics_port=8000)\n    \n    # Start Flask application\n    app.run(host=\'0.0.0.0\', port=5000, debug=False)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"free-resources",children:"Free Resources"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://prometheus.io/docs/",children:"Prometheus Documentation"})," - Complete monitoring solution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://grafana.com/docs/",children:"Grafana Documentation"})," - Visualization and dashboards"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.jaegertracing.io/docs/",children:"Jaeger Documentation"})," - Distributed tracing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://opentelemetry.io/",children:"OpenTelemetry"})," - Observability framework"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"2-devsecops-and-security-automation",children:"2. DevSecOps and Security Automation"}),"\n",(0,i.jsx)(n.h3,{id:"security-in-the-development-lifecycle",children:"Security in the Development Lifecycle"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Automated Security Pipeline:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# .github/workflows/security-pipeline.yml\nname: DevSecOps Security Pipeline\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      security-events: write\n      \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      # Static Application Security Testing (SAST)\n      - name: Run CodeQL Analysis\n        uses: github/codeql-action/init@v2\n        with:\n          languages: javascript, python\n          queries: security-extended,security-and-quality\n\n      - name: Autobuild\n        uses: github/codeql-action/autobuild@v2\n\n      - name: Perform CodeQL Analysis\n        uses: github/codeql-action/analyze@v2\n\n      # Secret scanning\n      - name: Run TruffleHog OSS\n        uses: trufflesecurity/trufflehog@main\n        with:\n          path: ./\n          base: main\n          head: HEAD\n          extra_args: --debug --only-verified\n\n      # Dependency scanning\n      - name: Run Snyk to check for vulnerabilities\n        uses: snyk/actions/node@master\n        env:\n          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n        with:\n          args: --severity-threshold=high\n\n      # Infrastructure as Code scanning\n      - name: Run Checkov\n        uses: bridgecrewio/checkov-action@master\n        with:\n          directory: .\n          framework: terraform,kubernetes,dockerfile\n          output_format: sarif\n          output_file_path: checkov-results.sarif\n\n      - name: Upload Checkov results to GitHub Security tab\n        uses: github/codeql-action/upload-sarif@v2\n        if: always()\n        with:\n          sarif_file: checkov-results.sarif\n\n      # License compliance\n      - name: FOSSA Scan\n        uses: fossa-contrib/fossa-action@v2\n        with:\n          api-key: ${{ secrets.FOSSA_API_KEY }}\n\n  container-security:\n    runs-on: ubuntu-latest\n    needs: security-scan\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Build Docker image\n        run: |\n          docker build -t ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} .\n\n      # Container vulnerability scanning\n      - name: Run Trivy vulnerability scanner\n        uses: aquasecurity/trivy-action@master\n        with:\n          image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}\n          format: 'sarif'\n          output: 'trivy-results.sarif'\n\n      - name: Upload Trivy scan results\n        uses: github/codeql-action/upload-sarif@v2\n        if: always()\n        with:\n          sarif_file: 'trivy-results.sarif'\n\n      # Container configuration scanning\n      - name: Run Dockle\n        run: |\n          curl -L -o dockle.deb https://github.com/goodwithtech/dockle/releases/download/v0.4.10/dockle_0.4.10_Linux-64bit.deb\n          sudo dpkg -i dockle.deb\n          dockle --exit-code 1 --exit-level warn ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}\n\n      # Runtime security scanning\n      - name: Run Falco rules check\n        run: |\n          docker run --rm -v $(pwd):/workspace falcosecurity/falco:latest \\\n            falco --dry-run -r /workspace/security/falco-rules.yaml\n\n  compliance-check:\n    runs-on: ubuntu-latest\n    \n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      # CIS Benchmark compliance\n      - name: Run Docker Bench Security\n        run: |\n          docker run --rm --net host --pid host --userns host --cap-add audit_control \\\n            -e DOCKER_CONTENT_TRUST=$DOCKER_CONTENT_TRUST \\\n            -v /etc:/etc:ro \\\n            -v /usr/bin/containerd:/usr/bin/containerd:ro \\\n            -v /usr/bin/runc:/usr/bin/runc:ro \\\n            -v /usr/lib/systemd:/usr/lib/systemd:ro \\\n            -v /var/lib:/var/lib:ro \\\n            -v /var/run/docker.sock:/var/run/docker.sock:ro \\\n            --label docker_bench_security \\\n            docker/docker-bench-security\n\n      # Kubernetes security scanning\n      - name: Run kube-score\n        run: |\n          curl -L https://github.com/zegl/kube-score/releases/download/v1.16.1/kube-score_1.16.1_linux_amd64.tar.gz | tar xz\n          ./kube-score score k8s/*.yaml\n\n      # Policy as Code validation\n      - name: Run OPA Conftest\n        run: |\n          curl -L https://github.com/open-policy-agent/conftest/releases/download/v0.46.0/conftest_0.46.0_Linux_x86_64.tar.gz | tar xz\n          ./conftest verify --policy security/policies k8s/*.yaml\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Security Policies as Code:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-rego",children:'# security/policies/kubernetes.rego\npackage kubernetes.security\n\n# Deny containers running as root\ndeny[msg] {\n    input.kind == "Deployment"\n    input.spec.template.spec.containers[_].securityContext.runAsUser == 0\n    msg := "Container must not run as root user"\n}\n\n# Require security context\ndeny[msg] {\n    input.kind == "Deployment"\n    container := input.spec.template.spec.containers[_]\n    not container.securityContext\n    msg := "Container must define securityContext"\n}\n\n# Require resource limits\ndeny[msg] {\n    input.kind == "Deployment"\n    container := input.spec.template.spec.containers[_]\n    not container.resources.limits\n    msg := "Container must define resource limits"\n}\n\n# Deny privileged containers\ndeny[msg] {\n    input.kind == "Deployment"\n    input.spec.template.spec.containers[_].securityContext.privileged == true\n    msg := "Privileged containers are not allowed"\n}\n\n# Require read-only root filesystem\ndeny[msg] {\n    input.kind == "Deployment"\n    container := input.spec.template.spec.containers[_]\n    not container.securityContext.readOnlyRootFilesystem == true\n    msg := "Container must have read-only root filesystem"\n}\n\n# Deny containers with capabilities\ndeny[msg] {\n    input.kind == "Deployment"\n    container := input.spec.template.spec.containers[_]\n    container.securityContext.capabilities.add\n    msg := "Containers must not add capabilities"\n}\n\n# Require non-root user\ndeny[msg] {\n    input.kind == "Deployment"\n    container := input.spec.template.spec.containers[_]\n    not container.securityContext.runAsNonRoot == true\n    msg := "Container must run as non-root user"\n}\n\n# Network policy requirements\ndeny[msg] {\n    input.kind == "Deployment"\n    not input.metadata.labels["network-policy"]\n    msg := "Deployment must have network-policy label"\n}\n\n# Image scanning requirements\ndeny[msg] {\n    input.kind == "Deployment"\n    container := input.spec.template.spec.containers[_]\n    not startswith(container.image, "registry.company.com/")\n    msg := "Container image must come from approved registry"\n}\n\n# Service account requirements\ndeny[msg] {\n    input.kind == "Deployment"\n    not input.spec.template.spec.serviceAccountName\n    msg := "Deployment must specify serviceAccountName"\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Runtime Security Monitoring:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# falco-rules.yaml - Runtime security rules\n- rule: Detect Shell in Container\n  desc: Detect shell execution in container\n  condition: >\n    spawned_process and container and\n    (proc.name in (shell_binaries) or\n     proc.name in (bash, sh, zsh, fish, csh, ksh))\n  output: >\n    Shell spawned in container (user=%user.name container_id=%container.id\n    container_name=%container.name shell=%proc.name parent=%proc.pname\n    cmdline=%proc.cmdline)\n  priority: WARNING\n  tags: [container, shell, mitre_execution]\n\n- rule: Detect Privilege Escalation\n  desc: Detect privilege escalation attempts\n  condition: >\n    spawned_process and container and\n    (proc.name in (sudo, su, doas) or\n     proc.cmdline contains "chmod +s" or\n     proc.cmdline contains "setuid")\n  output: >\n    Privilege escalation attempt detected (user=%user.name\n    container_id=%container.id container_name=%container.name\n    command=%proc.cmdline)\n  priority: HIGH\n  tags: [container, privilege_escalation, mitre_privilege_escalation]\n\n- rule: Detect Network Activity\n  desc: Detect unexpected network activity\n  condition: >\n    inbound_outbound and container and\n    not proc.name in (allowed_network_processes) and\n    not fd.sport in (allowed_ports) and\n    not fd.dport in (allowed_ports)\n  output: >\n    Unexpected network activity (user=%user.name container_id=%container.id\n    container_name=%container.name connection=%fd.name)\n  priority: WARNING\n  tags: [container, network, mitre_command_and_control]\n\n- rule: Detect File System Changes\n  desc: Detect unauthorized file system modifications\n  condition: >\n    open_write and container and\n    (fd.name startswith /etc or\n     fd.name startswith /usr/bin or\n     fd.name startswith /usr/sbin or\n     fd.name startswith /bin or\n     fd.name startswith /sbin) and\n    not proc.name in (allowed_system_processes)\n  output: >\n    Unauthorized file system modification (user=%user.name\n    container_id=%container.id container_name=%container.name\n    file=%fd.name command=%proc.cmdline)\n  priority: HIGH\n  tags: [container, filesystem, mitre_persistence]\n\n- rule: Detect Crypto Mining\n  desc: Detect potential cryptocurrency mining activity\n  condition: >\n    spawned_process and container and\n    (proc.name in (xmrig, cpuminer, cgminer, bfgminer) or\n     proc.cmdline contains "stratum+tcp" or\n     proc.cmdline contains "mining" or\n     proc.cmdline contains "cryptonight")\n  output: >\n    Potential crypto mining activity detected (user=%user.name\n    container_id=%container.id container_name=%container.name\n    command=%proc.cmdline)\n  priority: CRITICAL\n  tags: [container, crypto_mining, mitre_impact]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"secrets-management-and-encryption",children:"Secrets Management and Encryption"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"HashiCorp Vault Integration:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nSecrets management with HashiCorp Vault\n\"\"\"\nimport hvac\nimport os\nimport json\nimport base64\nfrom cryptography.fernet import Fernet\nfrom kubernetes import client, config\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass SecretsManager:\n    def __init__(self, vault_url=None, vault_token=None):\n        self.vault_url = vault_url or os.getenv('VAULT_ADDR', 'http://vault:8200')\n        self.vault_token = vault_token or os.getenv('VAULT_TOKEN')\n        \n        # Initialize Vault client\n        self.vault_client = hvac.Client(\n            url=self.vault_url,\n            token=self.vault_token\n        )\n        \n        # Initialize Kubernetes client\n        try:\n            config.load_incluster_config()\n        except:\n            config.load_kube_config()\n        \n        self.k8s_client = client.CoreV1Api()\n        \n        # Initialize encryption key\n        self.encryption_key = self._get_or_create_encryption_key()\n        self.cipher_suite = Fernet(self.encryption_key)\n    \n    def _get_or_create_encryption_key(self):\n        \"\"\"Get or create encryption key from Vault\"\"\"\n        try:\n            # Try to read existing key\n            response = self.vault_client.secrets.kv.v2.read_secret_version(\n                path='encryption/master-key'\n            )\n            return response['data']['data']['key'].encode()\n        except:\n            # Create new key if doesn't exist\n            key = Fernet.generate_key()\n            self.vault_client.secrets.kv.v2.create_or_update_secret(\n                path='encryption/master-key',\n                secret={'key': key.decode()}\n            )\n            return key\n    \n    def store_secret(self, path, secret_data, encrypt=True):\n        \"\"\"Store secret in Vault with optional encryption\"\"\"\n        try:\n            if encrypt:\n                # Encrypt sensitive data\n                encrypted_data = {}\n                for key, value in secret_data.items():\n                    if isinstance(value, str):\n                        encrypted_value = self.cipher_suite.encrypt(value.encode())\n                        encrypted_data[key] = base64.b64encode(encrypted_value).decode()\n                    else:\n                        encrypted_data[key] = value\n                \n                # Store encrypted data\n                self.vault_client.secrets.kv.v2.create_or_update_secret(\n                    path=path,\n                    secret=encrypted_data\n                )\n            else:\n                # Store unencrypted data\n                self.vault_client.secrets.kv.v2.create_or_update_secret(\n                    path=path,\n                    secret=secret_data\n                )\n            \n            logger.info(f\"Secret stored successfully at path: {path}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to store secret: {str(e)}\")\n            return False\n    \n    def retrieve_secret(self, path, decrypt=True):\n        \"\"\"Retrieve secret from Vault with optional decryption\"\"\"\n        try:\n            response = self.vault_client.secrets.kv.v2.read_secret_version(path=path)\n            secret_data = response['data']['data']\n            \n            if decrypt:\n                # Decrypt sensitive data\n                decrypted_data = {}\n                for key, value in secret_data.items():\n                    if isinstance(value, str):\n                        try:\n                            encrypted_value = base64.b64decode(value.encode())\n                            decrypted_value = self.cipher_suite.decrypt(encrypted_value)\n                            decrypted_data[key] = decrypted_value.decode()\n                        except:\n                            # Value might not be encrypted\n                            decrypted_data[key] = value\n                    else:\n                        decrypted_data[key] = value\n                \n                return decrypted_data\n            else:\n                return secret_data\n                \n        except Exception as e:\n            logger.error(f\"Failed to retrieve secret: {str(e)}\")\n            return None\n    \n    def create_k8s_secret(self, name, namespace, secret_data, secret_type='Opaque'):\n        \"\"\"Create Kubernetes secret from Vault data\"\"\"\n        try:\n            # Encode secret data\n            encoded_data = {}\n            for key, value in secret_data.items():\n                encoded_data[key] = base64.b64encode(value.encode()).decode()\n            \n            # Create secret object\n            secret = client.V1Secret(\n                metadata=client.V1ObjectMeta(\n                    name=name,\n                    namespace=namespace,\n                    annotations={\n                        'vault.hashicorp.com/agent-inject': 'true',\n                        'vault.hashicorp.com/role': 'myapp',\n                        'vault.hashicorp.com/agent-inject-secret-config': f'secret/data/{name}'\n                    }\n                ),\n                type=secret_type,\n                data=encoded_data\n            )\n            \n            # Create or update secret\n            try:\n                self.k8s_client.create_namespaced_secret(\n                    namespace=namespace,\n                    body=secret\n                )\n                logger.info(f\"Created Kubernetes secret: {name}\")\n            except client.ApiException as e:\n                if e.status == 409:  # Secret already exists\n                    self.k8s_client.patch_namespaced_secret(\n                        name=name,\n                        namespace=namespace,\n                        body=secret\n                    )\n                    logger.info(f\"Updated Kubernetes secret: {name}\")\n                else:\n                    raise\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to create Kubernetes secret: {str(e)}\")\n            return False\n    \n    def rotate_secret(self, path, new_secret_data):\n        \"\"\"Rotate secret with versioning\"\"\"\n        try:\n            # Store new version\n            self.store_secret(path, new_secret_data)\n            \n            # Get secret metadata\n            response = self.vault_client.secrets.kv.v2.read_secret_metadata(path=path)\n            current_version = response['data']['current_version']\n            \n            # Keep only last 5 versions\n            if current_version > 5:\n                versions_to_delete = list(range(1, current_version - 4))\n                self.vault_client.secrets.kv.v2.delete_secret_versions(\n                    path=path,\n                    versions=versions_to_delete\n                )\n            \n            logger.info(f\"Secret rotated successfully: {path}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to rotate secret: {str(e)}\")\n            return False\n    \n    def audit_secret_access(self, path):\n        \"\"\"Audit secret access logs\"\"\"\n        try:\n            # This would typically integrate with Vault's audit logs\n            # For demonstration, we'll show how to query audit data\n            \n            audit_data = {\n                'path': path,\n                'access_count': 0,\n                'last_accessed': None,\n                'accessed_by': []\n            }\n            \n            # In a real implementation, you would query Vault's audit logs\n            # or integrate with your logging system\n            \n            return audit_data\n            \n        except Exception as e:\n            logger.error(f\"Failed to audit secret access: {str(e)}\")\n            return None\n\n# Usage example\nif __name__ == \"__main__\":\n    secrets_manager = SecretsManager()\n    \n    # Store application secrets\n    app_secrets = {\n        'database_url': 'postgresql://user:password@db:5432/myapp',\n        'api_key': 'sk-1234567890abcdef',\n        'jwt_secret': 'super-secret-jwt-key'\n    }\n    \n    secrets_manager.store_secret('myapp/production', app_secrets)\n    \n    # Retrieve secrets\n    retrieved_secrets = secrets_manager.retrieve_secret('myapp/production')\n    print(\"Retrieved secrets:\", retrieved_secrets)\n    \n    # Create Kubernetes secret\n    secrets_manager.create_k8s_secret(\n        name='myapp-secrets',\n        namespace='production',\n        secret_data=retrieved_secrets\n    )\n    \n    # Rotate secrets\n    new_secrets = app_secrets.copy()\n    new_secrets['api_key'] = 'sk-new-key-0987654321'\n    secrets_manager.rotate_secret('myapp/production', new_secrets)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"free-resources-1",children:"Free Resources"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://owasp.org/www-project-devsecops-guideline/",children:"OWASP DevSecOps Guideline"})," - DevSecOps best practices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.vaultproject.io/",children:"HashiCorp Vault"})," - Secrets management"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://falco.org/",children:"Falco Security Monitoring"})," - Runtime security monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.openpolicyagent.org/",children:"Open Policy Agent"})," - Policy as code"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"3-site-reliability-engineering-sre",children:"3. Site Reliability Engineering (SRE)"}),"\n",(0,i.jsx)(n.h3,{id:"sre-principles-and-practices",children:"SRE Principles and Practices"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Service Level Objectives (SLOs) and Error Budgets:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSRE Service Level Objectives and Error Budget Management\n"""\nimport time\nimport json\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, asdict\nfrom typing import List, Dict, Optional\nimport requests\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass SLI:\n    """Service Level Indicator"""\n    name: str\n    description: str\n    query: str\n    unit: str\n    good_threshold: float\n    \n@dataclass\nclass SLO:\n    """Service Level Objective"""\n    name: str\n    description: str\n    sli: SLI\n    target: float  # e.g., 99.9 for 99.9%\n    window: str    # e.g., "30d" for 30 days\n    \n@dataclass\nclass ErrorBudget:\n    """Error Budget calculation"""\n    slo_name: str\n    target: float\n    actual: float\n    budget_remaining: float\n    budget_consumed: float\n    is_exhausted: bool\n    time_to_exhaustion: Optional[str]\n\nclass SREMonitor:\n    def __init__(self, prometheus_url: str):\n        self.prometheus_url = prometheus_url\n        self.slis = self._define_slis()\n        self.slos = self._define_slos()\n    \n    def _define_slis(self) -> List[SLI]:\n        """Define Service Level Indicators"""\n        return [\n            SLI(\n                name="availability",\n                description="Percentage of successful requests",\n                query="""\n                (\n                  sum(rate(http_requests_total{status!~"5.."}[5m])) /\n                  sum(rate(http_requests_total[5m]))\n                ) * 100\n                """,\n                unit="percentage",\n                good_threshold=99.0\n            ),\n            SLI(\n                name="latency_p99",\n                description="99th percentile response time",\n                query="""\n                histogram_quantile(0.99,\n                  sum(rate(http_request_duration_seconds_bucket[5m])) by (le)\n                ) * 1000\n                """,\n                unit="milliseconds",\n                good_threshold=500.0\n            ),\n            SLI(\n                name="error_rate",\n                description="Percentage of error responses",\n                query="""\n                (\n                  sum(rate(http_requests_total{status=~"5.."}[5m])) /\n                  sum(rate(http_requests_total[5m]))\n                ) * 100\n                """,\n                unit="percentage",\n                good_threshold=1.0\n            ),\n            SLI(\n                name="throughput",\n                description="Requests per second",\n                query="sum(rate(http_requests_total[5m]))",\n                unit="requests/second",\n                good_threshold=100.0\n            )\n        ]\n    \n    def _define_slos(self) -> List[SLO]:\n        """Define Service Level Objectives"""\n        sli_map = {sli.name: sli for sli in self.slis}\n        \n        return [\n            SLO(\n                name="availability_slo",\n                description="Service availability must be >= 99.9%",\n                sli=sli_map["availability"],\n                target=99.9,\n                window="30d"\n            ),\n            SLO(\n                name="latency_slo",\n                description="99th percentile latency must be <= 500ms",\n                sli=sli_map["latency_p99"],\n                target=500.0,\n                window="30d"\n            ),\n            SLO(\n                name="error_rate_slo",\n                description="Error rate must be <= 0.1%",\n                sli=sli_map["error_rate"],\n                target=0.1,\n                window="30d"\n            )\n        ]\n    \n    def query_prometheus(self, query: str, time_range: str = "30d") -> Dict:\n        """Query Prometheus for metrics"""\n        try:\n            # Calculate time range\n            end_time = datetime.now()\n            if time_range.endswith(\'d\'):\n                days = int(time_range[:-1])\n                start_time = end_time - timedelta(days=days)\n            elif time_range.endswith(\'h\'):\n                hours = int(time_range[:-1])\n                start_time = end_time - timedelta(hours=hours)\n            else:\n                start_time = end_time - timedelta(days=30)\n            \n            # Query Prometheus\n            params = {\n                \'query\': query,\n                \'start\': start_time.isoformat(),\n                \'end\': end_time.isoformat(),\n                \'step\': \'5m\'\n            }\n            \n            response = requests.get(\n                f"{self.prometheus_url}/api/v1/query_range",\n                params=params,\n                timeout=30\n            )\n            response.raise_for_status()\n            \n            return response.json()\n            \n        except Exception as e:\n            logger.error(f"Failed to query Prometheus: {str(e)}")\n            return {}\n    \n    def calculate_sli_value(self, sli: SLI, time_range: str = "30d") -> float:\n        """Calculate current SLI value"""\n        try:\n            result = self.query_prometheus(sli.query, time_range)\n            \n            if result.get(\'status\') == \'success\' and result.get(\'data\', {}).get(\'result\'):\n                # Get the latest value\n                values = result[\'data\'][\'result\'][0][\'values\']\n                if values:\n                    latest_value = float(values[-1][1])\n                    return latest_value\n            \n            return 0.0\n            \n        except Exception as e:\n            logger.error(f"Failed to calculate SLI value for {sli.name}: {str(e)}")\n            return 0.0\n    \n    def calculate_error_budget(self, slo: SLO) -> ErrorBudget:\n        """Calculate error budget for an SLO"""\n        try:\n            # Get current SLI value\n            current_value = self.calculate_sli_value(slo.sli, slo.window)\n            \n            # Calculate error budget\n            if slo.sli.name in ["availability"]:\n                # For availability, higher is better\n                actual_performance = current_value\n                target_performance = slo.target\n                \n                if actual_performance >= target_performance:\n                    budget_remaining = 100.0\n                    budget_consumed = 0.0\n                else:\n                    budget_consumed = ((target_performance - actual_performance) / \n                                     (100 - target_performance)) * 100\n                    budget_remaining = max(0, 100 - budget_consumed)\n                    \n            elif slo.sli.name in ["error_rate"]:\n                # For error rate, lower is better\n                actual_performance = current_value\n                target_performance = slo.target\n                \n                if actual_performance <= target_performance:\n                    budget_remaining = 100.0\n                    budget_consumed = 0.0\n                else:\n                    budget_consumed = (actual_performance / target_performance) * 100\n                    budget_remaining = max(0, 100 - budget_consumed)\n                    \n            elif slo.sli.name in ["latency_p99"]:\n                # For latency, lower is better\n                actual_performance = current_value\n                target_performance = slo.target\n                \n                if actual_performance <= target_performance:\n                    budget_remaining = 100.0\n                    budget_consumed = 0.0\n                else:\n                    budget_consumed = ((actual_performance - target_performance) / \n                                     target_performance) * 100\n                    budget_remaining = max(0, 100 - budget_consumed)\n            else:\n                budget_remaining = 100.0\n                budget_consumed = 0.0\n            \n            is_exhausted = budget_remaining <= 0\n            \n            # Estimate time to exhaustion (simplified)\n            time_to_exhaustion = None\n            if budget_remaining > 0 and budget_consumed > 0:\n                # This is a simplified calculation\n                consumption_rate = budget_consumed / 30  # per day\n                if consumption_rate > 0:\n                    days_remaining = budget_remaining / consumption_rate\n                    time_to_exhaustion = f"{days_remaining:.1f} days"\n            \n            return ErrorBudget(\n                slo_name=slo.name,\n                target=slo.target,\n                actual=current_value,\n                budget_remaining=budget_remaining,\n                budget_consumed=budget_consumed,\n                is_exhausted=is_exhausted,\n                time_to_exhaustion=time_to_exhaustion\n            )\n            \n        except Exception as e:\n            logger.error(f"Failed to calculate error budget for {slo.name}: {str(e)}")\n            return ErrorBudget(\n                slo_name=slo.name,\n                target=slo.target,\n                actual=0.0,\n                budget_remaining=0.0,\n                budget_consumed=100.0,\n                is_exhausted=True,\n                time_to_exhaustion=None\n            )\n    \n    def generate_sre_report(self) -> Dict:\n        """Generate comprehensive SRE report"""\n        report = {\n            \'timestamp\': datetime.now().isoformat(),\n            \'slis\': [],\n            \'slos\': [],\n            \'error_budgets\': [],\n            \'alerts\': [],\n            \'recommendations\': []\n        }\n        \n        # Calculate SLI values\n        for sli in self.slis:\n            current_value = self.calculate_sli_value(sli)\n            report[\'slis\'].append({\n                \'name\': sli.name,\n                \'description\': sli.description,\n                \'current_value\': current_value,\n                \'unit\': sli.unit,\n                \'good_threshold\': sli.good_threshold,\n                \'status\': \'good\' if current_value >= sli.good_threshold else \'bad\'\n            })\n        \n        # Calculate SLO compliance and error budgets\n        for slo in self.slos:\n            error_budget = self.calculate_error_budget(slo)\n            \n            report[\'slos\'].append({\n                \'name\': slo.name,\n                \'description\': slo.description,\n                \'target\': slo.target,\n                \'actual\': error_budget.actual,\n                \'window\': slo.window,\n                \'status\': \'compliant\' if not error_budget.is_exhausted else \'violated\'\n            })\n            \n            report[\'error_budgets\'].append(asdict(error_budget))\n            \n            # Generate alerts\n            if error_budget.is_exhausted:\n                report[\'alerts\'].append({\n                    \'severity\': \'critical\',\n                    \'message\': f"Error budget exhausted for {slo.name}",\n                    \'slo\': slo.name,\n                    \'budget_remaining\': error_budget.budget_remaining\n                })\n            elif error_budget.budget_remaining < 10:\n                report[\'alerts\'].append({\n                    \'severity\': \'warning\',\n                    \'message\': f"Error budget low for {slo.name}",\n                    \'slo\': slo.name,\n                    \'budget_remaining\': error_budget.budget_remaining\n                })\n        \n        # Generate recommendations\n        for error_budget in report[\'error_budgets\']:\n            if error_budget[\'is_exhausted\']:\n                report[\'recommendations\'].append({\n                    \'priority\': \'high\',\n                    \'action\': f"Immediate attention required for {error_budget[\'slo_name\']}",\n                    \'description\': "Error budget is exhausted. Consider implementing emergency measures."\n                })\n            elif error_budget[\'budget_remaining\'] < 25:\n                report[\'recommendations\'].append({\n                    \'priority\': \'medium\',\n                    \'action\': f"Monitor {error_budget[\'slo_name\']} closely",\n                    \'description\': "Error budget is running low. Review recent changes and performance."\n                })\n        \n        return report\n    \n    def should_halt_deployments(self) -> bool:\n        """Determine if deployments should be halted based on error budgets"""\n        for slo in self.slos:\n            error_budget = self.calculate_error_budget(slo)\n            if error_budget.is_exhausted:\n                logger.warning(f"Deployment halt recommended: {slo.name} error budget exhausted")\n                return True\n        return False\n\n# Usage example\nif __name__ == "__main__":\n    sre_monitor = SREMonitor("http://prometheus:9090")\n    \n    # Generate SRE report\n    report = sre_monitor.generate_sre_report()\n    print(json.dumps(report, indent=2))\n    \n    # Check if deployments should be halted\n    if sre_monitor.should_halt_deployments():\n        print("\ud83d\udea8 DEPLOYMENT HALT RECOMMENDED - Error budgets exhausted")\n    else:\n        print("\u2705 Deployments can proceed - Error budgets healthy")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"incident-management-and-post-mortems",children:"Incident Management and Post-Mortems"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Automated Incident Response:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nAutomated Incident Management System\n"""\nimport json\nimport time\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Optional\nimport requests\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass IncidentSeverity(Enum):\n    LOW = "low"\n    MEDIUM = "medium"\n    HIGH = "high"\n    CRITICAL = "critical"\n\nclass IncidentStatus(Enum):\n    OPEN = "open"\n    INVESTIGATING = "investigating"\n    IDENTIFIED = "identified"\n    MONITORING = "monitoring"\n    RESOLVED = "resolved"\n    CLOSED = "closed"\n\n@dataclass\nclass IncidentAction:\n    timestamp: datetime\n    action: str\n    user: str\n    details: str\n\n@dataclass\nclass Incident:\n    id: str\n    title: str\n    description: str\n    severity: IncidentSeverity\n    status: IncidentStatus\n    created_at: datetime\n    updated_at: datetime\n    resolved_at: Optional[datetime] = None\n    affected_services: List[str] = field(default_factory=list)\n    actions: List[IncidentAction] = field(default_factory=list)\n    metrics: Dict = field(default_factory=dict)\n    root_cause: Optional[str] = None\n    resolution: Optional[str] = None\n\nclass IncidentManager:\n    def __init__(self, config: Dict):\n        self.config = config\n        self.incidents: Dict[str, Incident] = {}\n        self.notification_channels = config.get(\'notification_channels\', {})\n        self.escalation_rules = config.get(\'escalation_rules\', {})\n        \n    def create_incident(self, \n                       title: str, \n                       description: str, \n                       severity: IncidentSeverity,\n                       affected_services: List[str] = None) -> Incident:\n        """Create a new incident"""\n        incident_id = f"INC-{int(time.time())}"\n        \n        incident = Incident(\n            id=incident_id,\n            title=title,\n            description=description,\n            severity=severity,\n            status=IncidentStatus.OPEN,\n            created_at=datetime.now(),\n            updated_at=datetime.now(),\n            affected_services=affected_services or []\n        )\n        \n        self.incidents[incident_id] = incident\n        \n        # Log incident creation\n        self._add_action(incident, "created", "system", f"Incident created with severity {severity.value}")\n        \n        # Send notifications\n        self._send_notifications(incident, "Incident Created")\n        \n        # Auto-escalate critical incidents\n        if severity == IncidentSeverity.CRITICAL:\n            self._escalate_incident(incident)\n        \n        logger.info(f"Created incident {incident_id}: {title}")\n        return incident\n    \n    def update_incident_status(self, incident_id: str, status: IncidentStatus, user: str = "system") -> bool:\n        """Update incident status"""\n        if incident_id not in self.incidents:\n            return False\n        \n        incident = self.incidents[incident_id]\n        old_status = incident.status\n        incident.status = status\n        incident.updated_at = datetime.now()\n        \n        if status == IncidentStatus.RESOLVED:\n            incident.resolved_at = datetime.now()\n        \n        self._add_action(incident, "status_changed", user, f"Status changed from {old_status.value} to {status.value}")\n        \n        # Send status update notifications\n        self._send_notifications(incident, f"Status Updated: {status.value}")\n        \n        logger.info(f"Updated incident {incident_id} status to {status.value}")\n        return True\n    \n    def add_incident_note(self, incident_id: str, note: str, user: str) -> bool:\n        """Add a note to an incident"""\n        if incident_id not in self.incidents:\n            return False\n        \n        incident = self.incidents[incident_id]\n        incident.updated_at = datetime.now()\n        \n        self._add_action(incident, "note_added", user, note)\n        \n        logger.info(f"Added note to incident {incident_id}")\n        return True\n    \n    def resolve_incident(self, incident_id: str, resolution: str, user: str) -> bool:\n        """Resolve an incident"""\n        if incident_id not in self.incidents:\n            return False\n        \n        incident = self.incidents[incident_id]\n        incident.status = IncidentStatus.RESOLVED\n        incident.resolved_at = datetime.now()\n        incident.updated_at = datetime.now()\n        incident.resolution = resolution\n        \n        self._add_action(incident, "resolved", user, f"Incident resolved: {resolution}")\n        \n        # Send resolution notifications\n        self._send_notifications(incident, "Incident Resolved")\n        \n        # Generate post-mortem if critical\n        if incident.severity in [IncidentSeverity.CRITICAL, IncidentSeverity.HIGH]:\n            self._generate_postmortem_template(incident)\n        \n        logger.info(f"Resolved incident {incident_id}")\n        return True\n    \n    def _add_action(self, incident: Incident, action: str, user: str, details: str):\n        """Add an action to incident timeline"""\n        action_obj = IncidentAction(\n            timestamp=datetime.now(),\n            action=action,\n            user=user,\n            details=details\n        )\n        incident.actions.append(action_obj)\n    \n    def _send_notifications(self, incident: Incident, event_type: str):\n        """Send incident notifications"""\n        try:\n            # Determine notification channels based on severity\n            channels = []\n            if incident.severity == IncidentSeverity.CRITICAL:\n                channels = self.notification_channels.get(\'critical\', [])\n            elif incident.severity == IncidentSeverity.HIGH:\n                channels = self.notification_channels.get(\'high\', [])\n            else:\n                channels = self.notification_channels.get(\'default\', [])\n            \n            message = self._format_notification_message(incident, event_type)\n            \n            for channel in channels:\n                if channel[\'type\'] == \'slack\':\n                    self._send_slack_notification(channel[\'webhook\'], message)\n                elif channel[\'type\'] == \'email\':\n                    self._send_email_notification(channel[\'recipients\'], message)\n                elif channel[\'type\'] == \'pagerduty\':\n                    self._send_pagerduty_notification(channel[\'integration_key\'], incident)\n        \n        except Exception as e:\n            logger.error(f"Failed to send notifications: {str(e)}")\n    \n    def _format_notification_message(self, incident: Incident, event_type: str) -> str:\n        """Format notification message"""\n        duration = ""\n        if incident.resolved_at and incident.created_at:\n            duration = str(incident.resolved_at - incident.created_at)\n        \n        message = f"""\n\ud83d\udea8 **{event_type}**\n\n**Incident ID:** {incident.id}\n**Title:** {incident.title}\n**Severity:** {incident.severity.value.upper()}\n**Status:** {incident.status.value.upper()}\n**Affected Services:** {\', \'.join(incident.affected_services)}\n**Created:** {incident.created_at.strftime(\'%Y-%m-%d %H:%M:%S\')}\n"""\n        \n        if incident.resolved_at:\n            message += f"**Resolved:** {incident.resolved_at.strftime(\'%Y-%m-%d %H:%M:%S\')}\\n"\n            message += f"**Duration:** {duration}\\n"\n        \n        if incident.resolution:\n            message += f"**Resolution:** {incident.resolution}\\n"\n        \n        return message\n    \n    def _send_slack_notification(self, webhook_url: str, message: str):\n        """Send Slack notification"""\n        try:\n            payload = {\n                \'text\': message,\n                \'username\': \'Incident Manager\',\n                \'icon_emoji\': \':rotating_light:\'\n            }\n            \n            response = requests.post(webhook_url, json=payload, timeout=10)\n            response.raise_for_status()\n            \n        except Exception as e:\n            logger.error(f"Failed to send Slack notification: {str(e)}")\n    \n    def _send_pagerduty_notification(self, integration_key: str, incident: Incident):\n        """Send PagerDuty notification"""\n        try:\n            payload = {\n                \'routing_key\': integration_key,\n                \'event_action\': \'trigger\' if incident.status != IncidentStatus.RESOLVED else \'resolve\',\n                \'dedup_key\': incident.id,\n                \'payload\': {\n                    \'summary\': incident.title,\n                    \'severity\': incident.severity.value,\n                    \'source\': \'incident-manager\',\n                    \'custom_details\': {\n                        \'incident_id\': incident.id,\n                        \'affected_services\': incident.affected_services,\n                        \'description\': incident.description\n                    }\n                }\n            }\n            \n            response = requests.post(\n                \'https://events.pagerduty.com/v2/enqueue\',\n                json=payload,\n                timeout=10\n            )\n            response.raise_for_status()\n            \n        except Exception as e:\n            logger.error(f"Failed to send PagerDuty notification: {str(e)}")\n    \n    def _escalate_incident(self, incident: Incident):\n        """Escalate incident based on rules"""\n        try:\n            escalation_rule = self.escalation_rules.get(incident.severity.value, {})\n            \n            if escalation_rule:\n                # Add escalation action\n                self._add_action(\n                    incident, \n                    "escalated", \n                    "system", \n                    f"Auto-escalated due to {incident.severity.value} severity"\n                )\n                \n                # Send escalation notifications\n                escalation_channels = escalation_rule.get(\'channels\', [])\n                for channel in escalation_channels:\n                    if channel[\'type\'] == \'pagerduty\':\n                        self._send_pagerduty_notification(channel[\'integration_key\'], incident)\n        \n        except Exception as e:\n            logger.error(f"Failed to escalate incident: {str(e)}")\n    \n    def _generate_postmortem_template(self, incident: Incident) -> str:\n        """Generate post-mortem template"""\n        template = f"""\n# Post-Mortem: {incident.title}\n\n**Incident ID:** {incident.id}\n**Date:** {incident.created_at.strftime(\'%Y-%m-%d\')}\n**Duration:** {incident.resolved_at - incident.created_at if incident.resolved_at else \'Ongoing\'}\n**Severity:** {incident.severity.value.upper()}\n\n## Summary\n{incident.description}\n\n## Timeline\n"""\n        \n        for action in incident.actions:\n            template += f"- **{action.timestamp.strftime(\'%H:%M:%S\')}** [{action.user}] {action.action}: {action.details}\\n"\n        \n        template += f"""\n\n## Root Cause\n{incident.root_cause or \'TBD - To be determined during post-mortem review\'}\n\n## Resolution\n{incident.resolution or \'TBD - To be documented\'}\n\n## Impact Assessment\n- **Affected Services:** {\', \'.join(incident.affected_services)}\n- **User Impact:** TBD\n- **Business Impact:** TBD\n\n## Action Items\n- [ ] Immediate fixes implemented\n- [ ] Monitoring improvements\n- [ ] Process improvements\n- [ ] Documentation updates\n- [ ] Training requirements\n\n## Lessons Learned\nTBD - To be filled during post-mortem meeting\n\n## Prevention Measures\nTBD - Actions to prevent similar incidents\n\n---\n*This post-mortem was auto-generated. Please review and complete all TBD sections.*\n"""\n        \n        # Save template to file or send to documentation system\n        filename = f"postmortem-{incident.id}-{incident.created_at.strftime(\'%Y%m%d\')}.md"\n        with open(filename, \'w\') as f:\n            f.write(template)\n        \n        logger.info(f"Generated post-mortem template: {filename}")\n        return template\n    \n    def get_incident_metrics(self, days: int = 30) -> Dict:\n        """Get incident metrics for reporting"""\n        cutoff_date = datetime.now() - timedelta(days=days)\n        recent_incidents = [\n            incident for incident in self.incidents.values()\n            if incident.created_at >= cutoff_date\n        ]\n        \n        metrics = {\n            \'total_incidents\': len(recent_incidents),\n            \'by_severity\': {},\n            \'by_status\': {},\n            \'mean_time_to_resolution\': 0,\n            \'incidents_by_service\': {},\n            \'resolution_rate\': 0\n        }\n        \n        # Count by severity\n        for severity in IncidentSeverity:\n            count = len([i for i in recent_incidents if i.severity == severity])\n            metrics[\'by_severity\'][severity.value] = count\n        \n        # Count by status\n        for status in IncidentStatus:\n            count = len([i for i in recent_incidents if i.status == status])\n            metrics[\'by_status\'][status.value] = count\n        \n        # Calculate MTTR\n        resolved_incidents = [\n            i for i in recent_incidents \n            if i.status == IncidentStatus.RESOLVED and i.resolved_at\n        ]\n        \n        if resolved_incidents:\n            total_resolution_time = sum([\n                (i.resolved_at - i.created_at).total_seconds()\n                for i in resolved_incidents\n            ])\n            metrics[\'mean_time_to_resolution\'] = total_resolution_time / len(resolved_incidents) / 3600  # hours\n            metrics[\'resolution_rate\'] = len(resolved_incidents) / len(recent_incidents) * 100\n        \n        # Count by affected service\n        for incident in recent_incidents:\n            for service in incident.affected_services:\n                metrics[\'incidents_by_service\'][service] = metrics[\'incidents_by_service\'].get(service, 0) + 1\n        \n        return metrics\n\n# Usage example\nif __name__ == "__main__":\n    config = {\n        \'notification_channels\': {\n            \'critical\': [\n                {\'type\': \'slack\', \'webhook\': \'https://hooks.slack.com/services/...\'},\n                {\'type\': \'pagerduty\', \'integration_key\': \'your-pagerduty-key\'}\n            ],\n            \'high\': [\n                {\'type\': \'slack\', \'webhook\': \'https://hooks.slack.com/services/...\'}\n            ],\n            \'default\': [\n                {\'type\': \'email\', \'recipients\': [\'team@company.com\']}\n            ]\n        },\n        \'escalation_rules\': {\n            \'critical\': {\n                \'channels\': [\n                    {\'type\': \'pagerduty\', \'integration_key\': \'escalation-key\'}\n                ]\n            }\n        }\n    }\n    \n    incident_manager = IncidentManager(config)\n    \n    # Create a critical incident\n    incident = incident_manager.create_incident(\n        title="Database Connection Pool Exhausted",\n        description="All database connections are in use, causing 500 errors",\n        severity=IncidentSeverity.CRITICAL,\n        affected_services=["user-service", "order-service"]\n    )\n    \n    # Update incident status\n    incident_manager.update_incident_status(incident.id, IncidentStatus.INVESTIGATING, "john.doe")\n    \n    # Add investigation notes\n    incident_manager.add_incident_note(\n        incident.id, \n        "Identified connection leak in user-service. Deploying fix.", \n        "jane.smith"\n    )\n    \n    # Resolve incident\n    incident_manager.resolve_incident(\n        incident.id,\n        "Fixed connection leak and increased pool size from 10 to 20 connections",\n        "jane.smith"\n    )\n    \n    # Get metrics\n    metrics = incident_manager.get_incident_metrics()\n    print(json.dumps(metrics, indent=2))\n'})}),"\n",(0,i.jsx)(n.h3,{id:"free-resources-2",children:"Free Resources"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://sre.google/books/",children:"Google SRE Books"})," - Comprehensive SRE practices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://sre.google/workbook/",children:"SRE Workbook"})," - Practical SRE implementation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://prometheus.io/docs/alerting/latest/",children:"Prometheus Alerting"})," - Monitoring and alerting"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://response.pagerduty.com/",children:"PagerDuty Incident Response"})," - Incident management guide"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-1-complete-observability-stack",children:"Exercise 1: Complete Observability Stack"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Task:"})," Deploy and configure a comprehensive observability platform."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Metrics collection with Prometheus"}),"\n",(0,i.jsx)(n.li,{children:"Log aggregation with ELK stack"}),"\n",(0,i.jsx)(n.li,{children:"Distributed tracing with Jaeger"}),"\n",(0,i.jsx)(n.li,{children:"Custom dashboards and alerting"}),"\n",(0,i.jsx)(n.li,{children:"SLI/SLO monitoring and error budget tracking"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-2-devsecops-pipeline-implementation",children:"Exercise 2: DevSecOps Pipeline Implementation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Task:"})," Build a complete security-integrated CI/CD pipeline."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Static and dynamic security testing"}),"\n",(0,i.jsx)(n.li,{children:"Container vulnerability scanning"}),"\n",(0,i.jsx)(n.li,{children:"Secrets management integration"}),"\n",(0,i.jsx)(n.li,{children:"Policy as code enforcement"}),"\n",(0,i.jsx)(n.li,{children:"Runtime security monitoring"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-3-sre-practice-implementation",children:"Exercise 3: SRE Practice Implementation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Task:"})," Implement SRE practices for a production service."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Define SLIs, SLOs, and error budgets"}),"\n",(0,i.jsx)(n.li,{children:"Implement automated incident response"}),"\n",(0,i.jsx)(n.li,{children:"Create post-mortem processes"}),"\n",(0,i.jsx)(n.li,{children:"Build capacity planning and performance optimization"}),"\n",(0,i.jsx)(n.li,{children:"Establish on-call procedures and runbooks"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Design a comprehensive observability strategy for a microservices architecture."})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Implement a DevSecOps pipeline that integrates security at every stage."})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Create an SRE framework with SLOs, error budgets, and incident management."})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Design a performance optimization strategy for high-scale applications."})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Develop an advanced automation framework for complex operational workflows."})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"congratulations-",children:"Congratulations! \ud83c\udf89"}),"\n",(0,i.jsx)(n.p,{children:"You have completed the comprehensive DevOps Engineering learning path! You now have the knowledge and skills to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Build and manage"})," production-ready infrastructure and applications"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement"})," comprehensive CI/CD pipelines and automation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Design"})," secure, scalable, and observable systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Apply"})," SRE principles and practices for reliability"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lead"})," DevOps transformation initiatives"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps-in-your-devops-career",children:"Next Steps in Your DevOps Career"}),"\n",(0,i.jsx)(n.h3,{id:"immediate-actions",children:(0,i.jsx)(n.strong,{children:"Immediate Actions:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Build a portfolio"})," showcasing your DevOps projects and implementations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Contribute to open source"})," DevOps tools and projects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Join professional communities"})," and attend DevOps conferences"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pursue certifications"})," from major cloud providers and DevOps tools"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Start mentoring"})," others beginning their DevOps journey"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"career-advancement-opportunities",children:(0,i.jsx)(n.strong,{children:"Career Advancement Opportunities:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Senior DevOps Engineer"}),": Lead complex infrastructure and automation projects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Platform Engineer"}),": Build and maintain developer platforms and tooling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Site Reliability Engineer"}),": Focus on system reliability and performance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"DevOps Architect"}),": Design enterprise-scale DevOps solutions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Engineering Manager"}),": Lead DevOps teams and drive organizational transformation"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"continuous-learning",children:(0,i.jsx)(n.strong,{children:"Continuous Learning:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Stay updated with emerging technologies (AI/ML Ops, Edge Computing, Quantum Computing)"}),"\n",(0,i.jsx)(n.li,{children:"Explore specialized areas (FinOps, GreenOps, DataOps)"}),"\n",(0,i.jsx)(n.li,{children:"Develop business and leadership skills"}),"\n",(0,i.jsx)(n.li,{children:"Build expertise in specific industries or domains"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,i.jsx)(n.h3,{id:"professional-development",children:"Professional Development"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://devopsinstitute.com/",children:"DevOps Institute"})," - Professional development and certification"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.cncf.io/training/",children:"CNCF Training"})," - Cloud native computing education"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://training.linuxfoundation.org/",children:"Linux Foundation Training"})," - Open source technology training"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://aws.amazon.com/training/",children:"AWS Training"})," - Cloud platform expertise"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"communities-and-networking",children:"Communities and Networking"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://devopsdays.org/",children:"DevOps Days"})," - Global DevOps conference series"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://events.linuxfoundation.org/",children:"KubeCon + CloudNativeCon"})," - Cloud native computing conferences"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.usenix.org/conferences/srecon",children:"SREcon"})," - Site Reliability Engineering conference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.meetup.com/",children:"Local Meetups"})," - Regional DevOps and cloud native groups"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Your DevOps engineering journey is just beginning. The skills you've developed will enable you to build the future of software delivery and operations. Keep learning, keep building, and keep pushing the boundaries of what's possible!"})," \ud83d\ude80"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);